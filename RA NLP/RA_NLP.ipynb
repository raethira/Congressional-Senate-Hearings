{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import xml.etree.ElementTree\n",
    "e = xml.etree.ElementTree.parse(url)\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "y=BeautifulSoup(e)\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "r = requests.get(url)\n",
    "root = ET.fromstring(r.text)\n",
    "\n",
    "#from bs4 import BeautifulSoup\n",
    "\n",
    "#y=BeautifulSoup(r)\n",
    "\n",
    "print (r)\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "r = requests.get(url)\n",
    "root = ET.fromstring(r.text)\n",
    "\n",
    "\n",
    "dom = etree.parse(r)\n",
    "# load XSLT\n",
    "transform = etree.XSLT(etree.fromstring(XSL))\n",
    "\n",
    "# apply XSLT on loaded dom\n",
    "json_text = str(transform(dom))\n",
    "\n",
    "# json_text contains the data converted to JSON format.\n",
    "# you can use it with the JSON API. Example:\n",
    "data = json.loads(json_text)\n",
    "print(data)\n",
    "\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = ['CHRG-105hhrg40050']\n",
    "count = 0\n",
    "for jacket in df1['filename']:\n",
    "    \n",
    "   # try:\n",
    "        print (count)\n",
    "        url = 'https://api.govinfo.gov/packages/'+jacket+'/mods?&api_key=XNEgGxjbEszIMyIeni9xpgdkqy60QD5p9S4Vvdlc'\n",
    "\n",
    "        r = requests.get(url)\n",
    "\n",
    "        with open('data.xml', 'w') as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "        with open(\"data.xml\", 'r') as f:\n",
    "            xmlString = f.read()\n",
    "\n",
    "        #print (\"XML input (data.xml):\")\n",
    "        #print(xmlString)\n",
    "\n",
    "        jsonString = json.dumps(xmltodict.parse(xmlString), indent=4)\n",
    "\n",
    "        jsonObj = json.loads(jsonString)\n",
    "\n",
    "        #print(\"\\nJSON output(output.json):\")\n",
    "        #print(jsonString)\n",
    "\n",
    "        #with open(\"output.json\", 'w') as f:\n",
    "        #    f.write(jsonString)\n",
    "\n",
    "\n",
    "        witnesses = []\n",
    "        witness_count = 0\n",
    "        try:\n",
    "            if \"witness\" in jsonObj[\"mods\"][\"extension\"][2]:\n",
    "                for witness in (jsonObj[\"mods\"][\"extension\"][2][\"witness\"]):\n",
    "                    witnesses.append(witness+'\\n')\n",
    "                    witness_count += 1\n",
    "        except:\n",
    "            witnesses.append (\"Not found\\n\")\n",
    "\n",
    "        count = count + 1\n",
    "        \n",
    "        \n",
    "        print (\"\".join(witnesses))\n",
    "        \n",
    "        \n",
    "\n",
    "        with open(metadata_results,'r') as csvinput:\n",
    "            with open(metadata_results_new, 'a') as csvoutput:\n",
    "                writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                reader = csv.reader(csvinput)\n",
    "\n",
    "                all = []\n",
    "                row = next(reader)\n",
    "                row.append('Witnesses & Affiliattions')\n",
    "                all.append(row)\n",
    "\n",
    "                for row in reader:\n",
    "                    row.append(\"\".join(witnesses))\n",
    "                    all.append(row)\n",
    "\n",
    "                writer.writerows(all)\n",
    "        \n",
    "        if (count > 2):\n",
    "            break\n",
    "\n",
    "        \n",
    "    #except:\n",
    "        #count = count + 1\n",
    "     #   continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Congressional committee name:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (jsonObj[\"mods\"][\"name\"][0][\"namePart\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Witnesses:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "witness_count = 0\n",
    "if \"witness\" in jsonObj[\"mods\"][\"extension\"][2]:\n",
    "    for witness in (jsonObj[\"mods\"][\"extension\"][2][\"witness\"]):\n",
    "        print (witness)\n",
    "        witness_count += 1\n",
    "else:\n",
    "    print (\"No witness information found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Affiliations:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameAff = {}\n",
    "for name in (jsonObj[\"mods\"][\"name\"]):\n",
    "    if name[\"@type\"] == \"personal\" and \"affiliation\" in name:\n",
    "        nameAff[name['namePart']] = name['affiliation']\n",
    "     \n",
    "for i in nameAff.items():\n",
    "    print (i[0] + '\\t' + i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata_results\n",
    "# Committee number column  - from individual csv\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csv = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "\n",
    "committees = {}\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "\n",
    "sample_jackets = [ 'CHRG-115hhrg27211']\n",
    "count = 0\n",
    "for jacket in df1['filename']:\n",
    "    \n",
    "    try:\n",
    "        #print (count)\n",
    "        \n",
    "        #if (count > 50):\n",
    "        #    break\n",
    "            \n",
    "        count = count + 1\n",
    "        \n",
    "        df2 = pd.read_csv(results_csv+jacket+'.csv')\n",
    "        \n",
    "        committees[jacket] = df2['committees'].iloc[0]\n",
    "\n",
    "        \n",
    "    except:\n",
    "        count = count + 1\n",
    "        continue\n",
    "\n",
    "print (committees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata_results\n",
    "# Committee number column  - from individual csv\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csv = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = [ 'CHRG-115hhrg27211']\n",
    "count = 0\n",
    "        \n",
    "with open(metadata_results,'r') as csvinput:\n",
    "            with open(metadata_results_new, 'w') as csvoutput:\n",
    "                writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                reader = csv.reader(csvinput)\n",
    "\n",
    "                all = []\n",
    "                row = next(reader)\n",
    "                row.append('Committees')\n",
    "                all.append(row)\n",
    "\n",
    "                for row in reader:\n",
    "                    \n",
    "               \n",
    "                    try:\n",
    "               \n",
    "                        if ( not math.isnan(committees[row[5]]) ):\n",
    "                                row.append(committees[row[5]])\n",
    "                            \n",
    "                        else:\n",
    "                            row.append(\"-\")\n",
    "                    except:\n",
    "                                row.append(\"-\")\n",
    "                            \n",
    "                    all.append(row)\n",
    "\n",
    "                writer.writerows(all)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual CSVs\n",
    "# Affiliations\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = ['CHRG-115hhrg27211']\n",
    "count = 0\n",
    "\n",
    "files = set(os.listdir(results_csvs)) - set(os.listdir(results_csvs_new))\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        url = 'https://api.govinfo.gov/packages/'+file.strip()[:-4]+'/mods?&api_key=qv508dpECfRcX6wttIoMw63RT81NPRgkNpsU58c2'\n",
    "\n",
    "        #print (url)\n",
    "        r = requests.get(url)\n",
    "\n",
    "        with open('data.xml', 'w', encoding=\"utf8\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "        with open(\"data.xml\", 'r', encoding=\"utf8\") as f:\n",
    "            xmlString = f.read()\n",
    "\n",
    "        #print (\"XML input (data.xml):\")\n",
    "        #print(xmlString)\n",
    "\n",
    "        jsonString = json.dumps(xmltodict.parse(xmlString), indent=4)\n",
    "        jsonObj = json.loads(jsonString)\n",
    "\n",
    "        with open(results_csvs+file,'r', encoding=\"utf8\") as csvinput:\n",
    "                with open(results_csvs_new+file, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Full name')\n",
    "                    row.append('Affiliation')\n",
    "                    all.append(row)\n",
    "                    #print (row)\n",
    "                    #try:\n",
    "\n",
    "                    for row in reader:\n",
    "\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            if ( row[-1] == \"Yes\"):\n",
    "                                    row.append(\"\".join(row[5].split(\",\")[:2]).strip())\n",
    "                                    row.append(\"\".join(row[5].split(\",\")[2:]).strip())\n",
    "                            else:\n",
    "\n",
    "                                try:\n",
    "                                    nameAff = {}\n",
    "                                    for name in (jsonObj[\"mods\"][\"name\"]):\n",
    "                                        if name[\"@type\"] == \"personal\" and \"affiliation\" in name:\n",
    "                                            nameAff[name['namePart']] = name['affiliation']\n",
    "\n",
    "                                    added = False\n",
    "                                    for i in nameAff.items():\n",
    "                                        if (fuzz.token_sort_ratio(i[0], row[5].strip()) > 85):\n",
    "                                                row.append(i[0])\n",
    "                                                row.append(i[1])\n",
    "                                                added = True\n",
    "                                                break\n",
    "\n",
    "                                    if(not added):\n",
    "                                        row.append(row[5].strip())\n",
    "                                        row.append(\"-\")\n",
    "\n",
    "                                except:\n",
    "                                    row.append(row[5].strip())\n",
    "                                    row.append(\"-\")\n",
    "\n",
    "                        except:\n",
    "                                    row.append(row[5].strip())\n",
    "                                    row.append(\"-\")\n",
    "\n",
    "                        all.append(row)\n",
    "\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata_results\n",
    "# Witness names & Affiliations, Members of the congress\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = ['CHRG-115hhrg27211']\n",
    "count = 0\n",
    "\n",
    "#files = set(os.listdir(results_csvs)) - set(os.listdir(results_csvs_new))\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "        with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Witnesses')\n",
    "                    row.append('Members of the congress')\n",
    "                    row.append('File exists')\n",
    "                    all.append(row)\n",
    "                    #print (row)\n",
    "                    #try:\n",
    "\n",
    "                    for row in reader:\n",
    "                            \n",
    "                            #try:\n",
    "                            if (row[6].strip()+'.csv' in os.listdir(results_csvs)):\n",
    "                                print (row[6].strip()+'.csv')\n",
    "                                file = pd.read_csv(results_csvs + row[6].strip() +'.csv')\n",
    "                            \n",
    "                                #print (file.head())\n",
    "                                witnesses = []\n",
    "                                members = []\n",
    "                                \n",
    "                                \n",
    "                                for index, row1 in file.iterrows():\n",
    "                                    #print (row1['Witness'])\n",
    "                                    temp = ''\n",
    "                                    if (row1['Witness'].strip() == \"Yes\"):\n",
    "                                        if (str(row1['Full name']).strip() != 'NA' and str(row1['Full name']).strip() != '-' and str(row1['Full name']).strip() != ''):\n",
    "                                            temp = str(row1['Full name'])\n",
    "                                            if (str(row1['Affiliation']).strip() != 'NA' and str(row1['Affiliation']).strip() != '-' and str(row1['Affiliation']).strip() != ''):\n",
    "                                                temp += ' : ' + str(row1['Affiliation']).strip() + ';\\n'\n",
    "                                                witnesses.append(temp)\n",
    "                                            else:\n",
    "                                                witnesses.append(temp + ';\\n')\n",
    "                                    else:\n",
    "                                        if (str(row1['Full name']).strip() != 'NA' and str(row1['Full name']).strip() != '-' and str(row1['Full name']).strip() != ''):\n",
    "                                            temp = str(row1['Full name'])\n",
    "                                            if (str(row1['Affiliation']).strip() != 'NA' and str(row1['Affiliation']).strip() != '-' and str(row1['Affiliation']).strip() != ''):\n",
    "                                                temp += ' : ' + str(row1['Affiliation']).strip() + ';\\n'\n",
    "                                                members.append(temp)\n",
    "                                            else:\n",
    "                                                members.append(temp + ';\\n')\n",
    "                                \n",
    "                                #print (witnesses)\n",
    "                                \n",
    "                                witnesses = [x for x in witnesses if str(x) != 'nan;']\n",
    "                                members = [x for x in members if str(x) != 'nan;']\n",
    "                                \n",
    "                                witnesses = set(witnesses)\n",
    "                                members = set(members)\n",
    "\n",
    "                                \n",
    "                                if (len(witnesses) == 0):\n",
    "                                    row.append('-')\n",
    "                                else:\n",
    "                                    row.append(\"\".join(witnesses).strip())\n",
    "                                \n",
    "                                if (len(members) == 0):\n",
    "                                    row.append('-')\n",
    "                                else:\n",
    "                                    row.append(\"\".join(members).strip())\n",
    "                                \n",
    "                                row.append(\"Yes\")\n",
    "                                \n",
    "                                all.append(row)\n",
    "                            \n",
    "                            else:\n",
    "                                    row.append('-')\n",
    "                                    row.append('-')\n",
    "                                    row.append(\"No\")\n",
    "                                    all.append(row)\n",
    "                                    \n",
    "                            #except:\n",
    "                            #    row.append(\"-\")\n",
    "                            #    row.append(\"-\")\n",
    "                                \n",
    "                            #    all.append(row)\n",
    "                            #    continue\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPO agencies \n",
    "# Individual CSVs\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "gpo = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019.csv\"\n",
    "\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(gpo)\n",
    "agencies = []\n",
    "for i in (df['Agency']):\n",
    "    temp = i.replace('U.S.', 'United States')\n",
    "    temp = temp.replace('U.S', 'United States')\n",
    "    temp = temp.replace('Dep.', 'Department')\n",
    "    \n",
    "    agencies.append(temp)\n",
    "    \n",
    "    \n",
    "#print (set(agencies))      \n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "file = pd.read_csv(sample_csvs + 'CHRG-104hhrg37344' +'.csv')\n",
    "for index, row1 in file.iterrows():\n",
    "    if (row1['Witness'] == \"Yes\"):\n",
    "        max_score = 0\n",
    "        for i in (set(agencies)):\n",
    "            score = fuzz.token_set_ratio( i.lower(), row1['Affiliation'].lower())\n",
    "            if (score > max_score):\n",
    "                max_score = score\n",
    "                agency = i\n",
    "        print ( row1['Affiliation'] + ' : ' + agency + '\\t' + str(max_score))\n",
    "                \n",
    "                \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_results_new\n",
    "# Remove \"nan\"\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csv = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = [ 'CHRG-115hhrg27211']\n",
    "count = 0\n",
    "        \n",
    "with open(metadata_results,'r',encoding=\"utf8\") as csvinput:\n",
    "            with open(metadata_results_new, 'w',encoding=\"utf8\") as csvoutput:\n",
    "                writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                reader = csv.reader(csvinput)\n",
    "\n",
    "                all = []\n",
    "                row = next(reader)\n",
    "                \n",
    "                all.append(row)\n",
    "\n",
    "                for row in reader:\n",
    "                \n",
    "                    row[-2] = \"\\n\".join(  list(filter(None, row[-2].replace('nan;','').split(\"\\n\"))) )\n",
    "                          \n",
    "                    if(row[-2].strip() == ''):\n",
    "                        row[-2] = '-'\n",
    "                        \n",
    "                    all.append(row)\n",
    "\n",
    "                writer.writerows(all)\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading API urls in json format to the local DB\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "#files = set(os.listdir(results_csvs)) - set(os.listdir(results_csvs_new))\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "\n",
    "sample_jackets = ['CHRG-105hhrg40050']\n",
    "count = 0\n",
    "for jacket in df1['Filename']:\n",
    "\n",
    "    try:\n",
    "        #print (set(os.listdir(APIs)))\n",
    "        #print (jacket+\".json\")\n",
    "        if jacket+\".json\" not in set(os.listdir(APIs)):\n",
    "            url = 'https://api.govinfo.gov/packages/'+jacket+'/mods?&api_key=XNEgGxjbEszIMyIeni9xpgdkqy60QD5p9S4Vvdlc'\n",
    "\n",
    "            r = requests.get(url)\n",
    "\n",
    "            with open('data.xml', 'w' , encoding=\"utf8\") as f:\n",
    "                f.write(r.text)\n",
    "\n",
    "            with open(\"data.xml\", 'r' , encoding=\"utf8\") as f:\n",
    "                xmlString = f.read()\n",
    "\n",
    "            #print (\"XML input (data.xml):\")\n",
    "            #print(xmlString)\n",
    "\n",
    "            jsonString = json.dumps(xmltodict.parse(xmlString), indent=4)\n",
    "\n",
    "            jsonObj = json.loads(jsonString)\n",
    "\n",
    "            #print(\"\\nJSON output(output.json):\")\n",
    "            #print(jsonString)\n",
    "\n",
    "            file = APIs + jacket+ \".json\"\n",
    "\n",
    "            with open(file, 'w', encoding=\"utf8\") as f:\n",
    "                f.write(jsonString)\n",
    "\n",
    "    except:\n",
    "        print(jacket)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading full text in .txt format to the local DB\n",
    "\n",
    "import os\n",
    "import urllib.request \n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "FullText = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/FullTexts/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "#files = set(os.listdir(results_csvs)) - set(os.listdir(results_csvs_new))\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "\n",
    "sample_jackets = ['CHRG-105hhrg40050']\n",
    "count = 0\n",
    "for jacket in df1['Filename']:\n",
    "\n",
    "    try:\n",
    "        #print (set(os.listdir(APIs)))\n",
    "        #print (jacket+\".json\")\n",
    "        if jacket+\".txt\" not in set(os.listdir(FullText)):\n",
    "            \n",
    "            url = 'https://api.govinfo.gov/packages/'+jacket+'/granules/'+jacket+'/htm?api_key=XNEgGxjbEszIMyIeni9xpgdkqy60QD5p9S4Vvdlc'\n",
    "                \n",
    "            file = FullText + jacket + \".txt\"\n",
    "            \n",
    "            urllib.request.urlretrieve(url, file)\n",
    "\n",
    "            \n",
    "    except:\n",
    "        print(jacket)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file in local DB\n",
    "\n",
    "file = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/FullTexts/CHRG-115hhrg23826.txt\"\n",
    "\n",
    "file_lines = open(file).readlines()\n",
    "print (file_lines[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual CSVs\n",
    "# heldDate extraction\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = ['CHRG-115hhrg27211']\n",
    "count = 0\n",
    "\n",
    "files = set(os.listdir(results_csvs)) - set(os.listdir(results_csvs_new))\n",
    "\n",
    "for file in os.listdir(results_csvs):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        #with open(APIs+file, 'r') as f:\n",
    "        #    xmlString = f.read()\n",
    "\n",
    "        #print (\"XML input (data.xml):\")\n",
    "        #print(xmlString)\n",
    "\n",
    "        file = file.replace('.csv','.json')\n",
    "        \n",
    "        with open(APIs+file) as data_file:    \n",
    "            jsonObj = json.load(data_file)\n",
    "        #print(jsonObj)    \n",
    "    \n",
    "        file = file.replace('.json','.csv')\n",
    "        \n",
    "     #   if file == 'CHRG-100shrg83712.csv' or file == 'CHRG-102hhrg67539.csv' or file == 'CHRG-103hhrg66111.csv'  :\n",
    "      #      continue\n",
    "        with open(results_csvs+file,'r', encoding=\"utf8\") as csvinput:\n",
    "                with open(results_csvs_new+file, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('heldDate')\n",
    "                    all.append(row)\n",
    "                    #print (row)\n",
    "                    #try:\n",
    "\n",
    "                    for row in reader:\n",
    "                        try:\n",
    "                            heldDate = []\n",
    "                            added = False\n",
    "                            exists = False\n",
    "                                    \n",
    "                            for item in (jsonObj[\"mods\"][\"extension\"]):\n",
    "                                        #for item in extension:\n",
    "                                            #print (item)\n",
    "                                            if \"heldDate\" in item:\n",
    "                                                exists = True\n",
    "                                                if isinstance(item[\"heldDate\"], list):\n",
    "                                                    for date in item[\"heldDate\"]:\n",
    "                                                        heldDate.append(date)\n",
    "                                                        added = True\n",
    "                                                        #print (heldDate)\n",
    "                                                else:\n",
    "                                                    row.append(item[\"heldDate\"])\n",
    "                                                    #print (item[\"heldDate\"])\n",
    "                                                    break\n",
    "                                   \n",
    "                            if exists == False:\n",
    "                                row.append(\"-\")\n",
    "                            if added:\n",
    "                                row.append(\";\\n\".join(heldDate))\n",
    "                                #break\n",
    "                          \n",
    "                        except:\n",
    "                            row.append(\"-\")\n",
    "\n",
    "                        all.append(row)\n",
    "\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)\n",
    "    except:\n",
    "        print (file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_results\n",
    "# heldDate extraction\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = ['CHRG-115hhrg27211']\n",
    "count = 0\n",
    "\n",
    "files = set(os.listdir(results_csvs)) - set(os.listdir(results_csvs_new))\n",
    "\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "\n",
    "                        all = []\n",
    "                        row = next(reader)\n",
    "\n",
    "                        row.append('heldDate')\n",
    "                        all.append(row)\n",
    "                        #print (row)\n",
    "                        #try:\n",
    "\n",
    "                        \n",
    "                        for row in reader:\n",
    "                            try:\n",
    "\n",
    "\n",
    "                                #if (row[6].strip()+'.csv' in os.listdir(results_csvs)):\n",
    "                                    #print (row[6].strip()+'.csv')\n",
    "                                file = row[6].strip()\n",
    "\n",
    "                                file = file + '.json'\n",
    "\n",
    "                                with open(APIs+file) as data_file:    \n",
    "                                        jsonObj = json.load(data_file)\n",
    "                                    #print(jsonObj)\n",
    "\n",
    "                                heldDate = []\n",
    "                                added = False\n",
    "                                exists = False\n",
    "\n",
    "                                for item in (jsonObj[\"mods\"][\"extension\"]):\n",
    "                                            #for item in extension:\n",
    "                                                #print (item)\n",
    "                                                if \"heldDate\" in item:\n",
    "                                                    exists = True\n",
    "                                                    if isinstance(item[\"heldDate\"], list):\n",
    "                                                        for date in item[\"heldDate\"]:\n",
    "                                                            heldDate.append(date)\n",
    "                                                            added = True\n",
    "                                                            #print (heldDate)\n",
    "                                                    else:\n",
    "                                                        row.append(item[\"heldDate\"])\n",
    "                                                       # print (item[\"heldDate\"])\n",
    "                                                        break\n",
    "\n",
    "                                if exists == False:\n",
    "                                    row.append(\"-\")\n",
    "                                if added:\n",
    "                                    row.append(\";\\n\".join(heldDate))\n",
    "                                    #break\n",
    "\n",
    "                            except:\n",
    "                                row.append(\"-\")\n",
    "\n",
    "                            all.append(row)\n",
    "\n",
    "                        #except:\n",
    "                        #    writer.writerows(all)\n",
    "                        #    continue\n",
    "                        writer.writerows(all)\n",
    "                    \n",
    "                    except:\n",
    "                        print (file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPO agencies for sample 500 CSVs\n",
    "# Individual CSVs\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "gpo = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019.csv\"\n",
    "gpo2 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019_v2.csv\"\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "sample500 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Sample_500_108th-112th_Congresses_1.31.19.csv\"\n",
    "\n",
    "sample500GPOOutput = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs500-GPOs/\"\n",
    "\n",
    "df1 = pd.read_csv(sample500)\n",
    "#print(df1['filename'])\n",
    "\n",
    "df = pd.read_csv(gpo2)\n",
    "agencies = []\n",
    "for i in (df['Agency']):\n",
    "    temp = i.replace('U.S.', 'United States')\n",
    "    temp = temp.replace('U.S', 'United States')\n",
    "    temp = temp.replace('Dep.', 'Department')\n",
    "    \n",
    "    agencies.append(temp)\n",
    "    \n",
    "    \n",
    "#print (set(agencies))      \n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#file = pd.read_csv(sample_csvs + 'CHRG-105hhrg40051' +'.csv')\n",
    "\n",
    "for file in df1['filename']:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "                #print ( row1['Affiliation'] + ' : ' + agency + '\\t' + str(max_score))\n",
    "            \n",
    "        with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "                    with open(sample500GPOOutput+file+'.csv', 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                        writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                        reader = csv.reader(csvinput)\n",
    "\n",
    "                        all = []\n",
    "                        row = next(reader)\n",
    "\n",
    "                        row.append('Government agencies')\n",
    "                        all.append(row)\n",
    "                        #print (row)\n",
    "                        #try:\n",
    "\n",
    "                        for row in reader:\n",
    "                            \n",
    "                            file1 = pd.read_csv(results_csvs + file +'.csv')\n",
    "    \n",
    "                            max_score = 0\n",
    "                            agency = '-'\n",
    "                            #print (row[18])\n",
    "                            if (str(row[16]).strip() == \"Yes\"):\n",
    "                                    max_score = 0\n",
    "                                    agency = '-'\n",
    "                                    for i in (set(agencies)):\n",
    "                                        score = fuzz.token_set_ratio( i.lower(), row[18].lower())\n",
    "                                        if (score > max_score):\n",
    "                                            max_score = score\n",
    "                                            agency = i\n",
    "                        \n",
    "                        \n",
    "                            if max_score == 100:\n",
    "                                row.append(agency)\n",
    "                            else:\n",
    "                                row.append(agency)\n",
    "\n",
    "                            all.append(row)\n",
    "\n",
    "                        #except:\n",
    "                        #    writer.writerows(all)\n",
    "                        #    continue\n",
    "                        writer.writerows(all)\n",
    "    except:\n",
    "        print (file)\n",
    "                \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis for sample 500 CSVs\n",
    "# Individual CSVs\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "gpo = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019.csv\"\n",
    "gpo2 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019_v2.csv\"\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "sample500 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Sample_500_108th-112th_Congresses_1.31.19.csv\"\n",
    "\n",
    "sample500GPOOutput = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs500-GPOs/\"\n",
    "\n",
    "sample500SAOutput = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs500-SA/\"\n",
    "\n",
    "df1 = pd.read_csv(sample500)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "#print (set(agencies))      \n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#file = pd.read_csv(sample_csvs + 'CHRG-105hhrg40051' +'.csv')\n",
    "\n",
    "for file in df1['filename']:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "                #print ( row1['Affiliation'] + ' : ' + agency + '\\t' + str(max_score))\n",
    "            \n",
    "        with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "                    with open(sample500SAOutput+file+'.csv', 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                        writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                        reader = csv.reader(csvinput)\n",
    "\n",
    "                        all = []\n",
    "                        row = next(reader)\n",
    "\n",
    "                        row.append('Sentiment analysis')\n",
    "                        all.append(row)\n",
    "                        #print (row)\n",
    "                        #try:\n",
    "                \n",
    "                        \n",
    "                        \n",
    "                        #print (ss)\n",
    "                        #print (max(ss, key=ss.get))\n",
    "                        #break\n",
    "                        for row in reader:\n",
    "                            \n",
    "                            #df2 = pd.read_csv(results_csvs+file+'.csv')\n",
    "                    \n",
    "                            #print (df2['cleaned'])\n",
    "                            ss = sid.polarity_scores(row[12])\n",
    "\n",
    "                            del (ss['compound'])\n",
    "                        \n",
    "                            #print (row[12])\n",
    "                            \n",
    "                            if ( max(ss, key=ss.get) == 'neu'):\n",
    "                                row.append('Neutral')\n",
    "                                \n",
    "                            \n",
    "                            if ( max(ss, key=ss.get) == 'neg'):\n",
    "                                row.append('Negative')\n",
    "                                \n",
    "                            \n",
    "                            if ( max(ss, key=ss.get) == 'pos'):\n",
    "                                row.append('Positive')\n",
    "                                \n",
    "                            all.append(row)\n",
    "\n",
    "                        #except:\n",
    "                        #    writer.writerows(all)\n",
    "                        #    continue\n",
    "                        writer.writerows(all)\n",
    "    except:\n",
    "        print (file)\n",
    "                \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata_results\n",
    "# Witness names & Affiliations, Members of the congress from FULL Texts  - Scrapped Witnesses\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "FullTexts = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/FullTexts/\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = ['CHRG-115hhrg27211']\n",
    "count = 0\n",
    "\n",
    "#files = set(os.listdir(results_csvs)) - set(os.listdir(results_csvs_new))\n",
    "\n",
    "countWitness = 0\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "        with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Scrapped witnesses')\n",
    "                    all.append(row)\n",
    "                    \n",
    "                    for row in reader:\n",
    "                            \n",
    "                            #try:\n",
    "                            if (row[9]!='Appropriation' and row[9]!='Nomination') and row[14]=='-' and row[16]=='Yes' and row[14]=='-':\n",
    "                                    \n",
    "                                    #print(row)\n",
    "                                    if row[6]+'.txt' in set(os.listdir(FullTexts)):\n",
    "                                        filename = FullTexts+row[6]+'.txt'\n",
    "                                        lines = open(filename, \"r\", encoding=\"utf8\").readlines()\n",
    "                                        \n",
    "                                        #print (lines)\n",
    "                                        strippedLines = []\n",
    "                                        for line in lines:\n",
    "                                            #print (line.strip())\n",
    "                                            strippedLines.append(line.strip())\n",
    "                                        \n",
    "                                        \n",
    "                                        if ('C O N T E N T S' in strippedLines and 'Statement of:' in strippedLines):\n",
    "                                                \n",
    "                                                \n",
    "                                                startingIndex = strippedLines.index('Statement of:')\n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+1, len(lines)):\n",
    "                                                    if '    ' in lines[i]:\n",
    "                                                        \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            continue\n",
    "                                                        \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$', '', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].strip().split(';')[0]+'\\n')\n",
    "                                                            witness.append(lines[i].strip().split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                    else:\n",
    "                                                        break\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                                \n",
    "                                    \n",
    "                                        elif ('C O N T E N T S' in strippedLines and 'STATEMENTS' in strippedLines):\n",
    "                                                #countWitness += 1\n",
    "                                                \n",
    "                                                startingIndex = strippedLines.index('STATEMENTS')\n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+1, len(lines)):\n",
    "                                                   \n",
    "                                                        if 'APPENDIX' in lines[i] or 'Appendix' in lines[i]:\n",
    "                                                            break\n",
    "                                                        if 'Page' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if lines[i].isupper():\n",
    "                                                            break\n",
    "                                                        \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            continue\n",
    "                                                \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$', '', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].split(';')[0].strip()+'\\n')\n",
    "                                                            witness.append(lines[i].split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                       \n",
    "                                        elif ('CONTENTS' in strippedLines and 'TESTIMONY' in strippedLines):\n",
    "                                                #countWitness += 1\n",
    "                                                \n",
    "                                                startingIndex = strippedLines.index('TESTIMONY')\n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+1, len(lines)):\n",
    "                                                   \n",
    "                                                        if 'APPENDIX' in lines[i] or 'Appendix' in lines[i]:\n",
    "                                                            break\n",
    "                                                        if 'Page' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if lines[i].isupper():\n",
    "                                                            break\n",
    "                                                        \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            continue\n",
    "                                                        \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$','', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].split(';')[0].strip()+'\\n')\n",
    "                                                            witness.append(lines[i].split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                    \n",
    "                                        elif ('C O N T E N T S' in strippedLines and 'Testimony of:' in strippedLines):\n",
    "                                                #countWitness += 1\n",
    "                                                \n",
    "                                                startingIndex = strippedLines.index('Testimony of:')\n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+1, len(lines)):\n",
    "                                                   \n",
    "                                                        if 'APPENDIX' in lines[i] or 'Appendix' in lines[i]:\n",
    "                                                            break\n",
    "                                                        if 'Page' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if lines[i].isupper():\n",
    "                                                            break\n",
    "                                                          \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$','', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].split(';')[0].strip()+'\\n')\n",
    "                                                            witness.append(lines[i].split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                                    \n",
    "                                        elif ('C O N T E N T S' in strippedLines and 'CHRONOLOGICAL LIST OF WITNESSES' in strippedLines):\n",
    "                                                #countWitness += 1\n",
    "                                                \n",
    "                                                startingIndex = strippedLines.index('CHRONOLOGICAL LIST OF WITNESSES')\n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+1, len(lines)):\n",
    "                                                   \n",
    "                                                                                                               \n",
    "                                                        if lines[i].isupper():\n",
    "                                                            break\n",
    "                                                          \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$','', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].split(';')[0].strip()+'\\n')\n",
    "                                                            witness.append(lines[i].split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                                    \n",
    "                                        elif ('C O N T E N T S' in strippedLines and ('Panel I' in strippedLines or 'PANEL I' in strippedLines)) :\n",
    "                                                #countWitness += 1\n",
    "                                                \n",
    "                                                if 'Panel I' in strippedLines:\n",
    "                                                    startingIndex = strippedLines.index('Panel I')\n",
    "                                                if 'PANEL I' in strippedLines:\n",
    "                                                    startingIndex = strippedLines.index('PANEL I')\n",
    "                                                    \n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+1, len(lines)):\n",
    "                                                   \n",
    "                                                        if lines[i].strip == '----------':\n",
    "                                                            break\n",
    "                                                            \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if 'Panel' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if 'APPENDIX' in lines[i] or 'Appendix' in lines[i]:\n",
    "                                                            break\n",
    "                                                        if 'Page' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if lines[i].isupper():\n",
    "                                                            break\n",
    "                                                   \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$','', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].split(';')[0].strip()+'\\n')\n",
    "                                                            witness.append(lines[i].split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                            \n",
    "                                        elif ('C O N T E N T S' in strippedLines and ('Participants' in strippedLines)) :\n",
    "                                                #countWitness += 1\n",
    "                                                \n",
    "                                                if 'Participants' in strippedLines:\n",
    "                                                    startingIndex = strippedLines.index('Participants')\n",
    "                                                    \n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+2, len(lines)):\n",
    "                                                   \n",
    "                                                        if lines[i].strip == '----------':\n",
    "                                                            break\n",
    "                                                            \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            break\n",
    "                                                            \n",
    "                                                        if 'Panel' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if 'APPENDIX' in lines[i] or 'Appendix' in lines[i]:\n",
    "                                                            break\n",
    "                                                        if 'Page' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if lines[i].isupper():\n",
    "                                                            break\n",
    "                                                   \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$','', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].split(';')[0].strip()+'\\n')\n",
    "                                                            witness.append(lines[i].split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                        \n",
    "                                        \n",
    "                                        elif ('C O N T E N T S' in strippedLines and 'Statements:' in strippedLines):\n",
    "                                                #countWitness += 1\n",
    "                                                \n",
    "                                                startingIndex = strippedLines.index('Statements:')\n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+1, len(lines)):\n",
    "                                                   \n",
    "                                                        if 'APPENDIX' in lines[i] or 'Appendix' in lines[i]:\n",
    "                                                            break\n",
    "                                                        if 'Page' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if lines[i].isupper():\n",
    "                                                            break\n",
    "                                                        \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            continue\n",
    "                                                \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$', '', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].split(';')[0].strip()+'\\n')\n",
    "                                                            witness.append(lines[i].split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                                    \n",
    "                                        elif ('C O N T E N T S' in strippedLines and 'WITNESS' in strippedLines):\n",
    "                                                #countWitness += 1\n",
    "                                                \n",
    "                                                startingIndex = strippedLines.index('WITNESS')\n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+1, len(lines)):\n",
    "                                                   \n",
    "                                                        if 'APPENDIX' in lines[i] or 'Appendix' in lines[i]:\n",
    "                                                            break\n",
    "                                                        if 'Page' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if lines[i].isupper():\n",
    "                                                            break\n",
    "                                                        \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            continue\n",
    "                                                \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$', '', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].split(';')[0].strip()+'\\n')\n",
    "                                                            witness.append(lines[i].split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                        \n",
    "                                        elif ('C O N T E N T S' in strippedLines and 'Witnesses:' in strippedLines):\n",
    "                                                #countWitness += 1\n",
    "                                                \n",
    "                                                startingIndex = strippedLines.index('Witnesses:')\n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+1, len(lines)):\n",
    "                                                   \n",
    "                                                        if 'APPENDIX' in lines[i] or 'Appendix' in lines[i]:\n",
    "                                                            break\n",
    "                                                        if 'Page' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if lines[i].isupper():\n",
    "                                                            break\n",
    "                                                        \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            continue\n",
    "                                                \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$', '', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].split(';')[0].strip()+'\\n')\n",
    "                                                            witness.append(lines[i].split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                        \n",
    "                                        elif ('THE FUTURE OF THE OSCE MEDITERRANEAN PARTNERS FOR COOPERATION' in strippedLines and 'WITNESSES' in strippedLines):\n",
    "                                                #countWitness += 1\n",
    "                                                \n",
    "                                                startingIndex = strippedLines.index('WITNESSES')\n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+1, len(lines)):\n",
    "                                                   \n",
    "                                                        if 'APPENDIX' in lines[i] or 'Appendix' in lines[i]:\n",
    "                                                            break\n",
    "                                                        if 'Page' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if lines[i].isupper():\n",
    "                                                            break\n",
    "                                                        \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            continue\n",
    "                                                \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$', '', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].split(';')[0].strip()+'\\n')\n",
    "                                                            witness.append(lines[i].split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                                    \n",
    "                                        elif ('C O N T E N T S' in strippedLines and 'Page' in strippedLines):\n",
    "                                                #countWitness += 1\n",
    "                                                \n",
    "                                                startingIndex = strippedLines.index('Page')\n",
    "                                                #print (startingIndex)\n",
    "                                                witness = []\n",
    "                                                \n",
    "                                                #print ('\\n'+row[6])\n",
    "                                                #print (lines)\n",
    "                                                witnessStr = []\n",
    "                                                firstHit = 0\n",
    "                                                for i in range(startingIndex+1, len(lines)):\n",
    "                                                   \n",
    "                                                        if 'APPENDIX' in lines[i] or 'Appendix' in lines[i]:\n",
    "                                                            break\n",
    "                                                        if 'Page' in lines[i]:\n",
    "                                                            continue\n",
    "                                                            \n",
    "                                                        if lines[i].isupper():\n",
    "                                                            break\n",
    "                                                        \n",
    "                                                        if lines[i].strip() == '':\n",
    "                                                            continue\n",
    "                                                \n",
    "                                                        if re.search(r\"\\.(\\.)+( *)[0-9]*(\\*)*$\",lines[i]):\n",
    "                                                            if(firstHit == 0):\n",
    "                                                                x = re.sub('\\.(\\.)+( *)[0-9]*(\\*)*$', '', lines[i])\n",
    "                                                                witness.append(x.strip()+'\\n')\n",
    "                                                                firstHit = 1\n",
    "                                                                \n",
    "                                                        elif ';' in lines[i]:\n",
    "                                                            witness.append(lines[i].split(';')[0].strip()+'\\n')\n",
    "                                                            witness.append(lines[i].split(';')[1].replace('and','').strip())\n",
    "                                                            firstHit = 0\n",
    "                                                            \n",
    "                                                        else:\n",
    "                                                            witness.append(lines[i].strip()+' ')\n",
    "                                                            firstHit = 0\n",
    "                                                        \n",
    "                                                    \n",
    "                                                #print (\"\".join(witness))\n",
    "                                                \n",
    "                                                if(len(\"\".join(witness)) < 6000):\n",
    "                                                    row.append(\"\".join(witness))\n",
    "                                                    row[14] = 'Refer column S'\n",
    "                                                    countWitness += 1\n",
    "                                                    \n",
    "                                        else:\n",
    "                                            row.append('-')\n",
    "                                            #row[14] = 'Refer column S'\n",
    "                            \n",
    "                            else:\n",
    "                                row.append('-')\n",
    "                                \n",
    "                            #if countWitness !=0:\n",
    "                            #    break        \n",
    "                            #except:\n",
    "                            #    row.append(\"-\")\n",
    "                            #    row.append(\"-\")\n",
    "                            all.append(row)    \n",
    "                            #    all.append(row)\n",
    "                            #    continue\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)\n",
    "\n",
    "print(countWitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata_results\n",
    "# Witness names & Affiliations, Members of the congress from FULL Texts - Scrapped Witnesses for individual CSVs\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "FullTexts = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/FullTexts/\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = ['CHRG-115hhrg27211']\n",
    "count = 0\n",
    "\n",
    "#files = set(os.listdir(results_csvs)) - set(os.listdir(results_csvs_new))\n",
    "\n",
    "countWitness = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gpo = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019.csv\"\n",
    "gpo2 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019_v2.csv\"\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "sample500 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Sample_500_108th-112th_Congresses_1.31.19.csv\"\n",
    "\n",
    "sample500GPOOutput = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs500-GPOs/\"\n",
    "\n",
    "sample500SAOutput = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs500-SA/\"\n",
    "\n",
    "#df1 = pd.read_csv(sample_csvs_new)\n",
    "#print(df1['filename'])\n",
    "\n",
    "#sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "#print (set(agencies))      \n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#file = pd.read_csv(sample_csvs + 'CHRG-105hhrg40051' +'.csv')\n",
    "\n",
    "scrappedWD = {}\n",
    "\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                    \n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    for row in reader:\n",
    "                            \n",
    "                            #try:\n",
    "                            if row[13] =='Refer column R' :\n",
    "                                    \n",
    "                                    scrappedWD[row[6]] = row[17]\n",
    "    \n",
    "for k, v in scrappedWD.items():\n",
    "    print (v.split('\\n'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata_results\n",
    "# Witness names & Affiliations, Members of the congress from FULL Texts - Scrapped Witnesses for individual CSVs\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "FullTexts = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/FullTexts/\"\n",
    "\n",
    "\n",
    "#df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = ['CHRG-115hhrg27211']\n",
    "count = 0\n",
    "\n",
    "#files = set(os.listdir(results_csvs)) - set(os.listdir(results_csvs_new))\n",
    "\n",
    "countWitness = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gpo = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019.csv\"\n",
    "gpo2 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019_v2.csv\"\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "sample500 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Sample_500_108th-112th_Congresses_1.31.19.csv\"\n",
    "\n",
    "sample500GPOOutput = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs500-GPOs/\"\n",
    "\n",
    "sample500SAOutput = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs500-SA/\"\n",
    "\n",
    "df1 = pd.read_csv(sample500)\n",
    "#print(df1['filename'])\n",
    "\n",
    "#sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "#print (set(agencies))      \n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#file = pd.read_csv(sample_csvs + 'CHRG-105hhrg40051' +'.csv')\n",
    "\n",
    "for file in set(os.listdir(results_csvs)):\n",
    "#for file in set(os.listdir(results_csvs)):\n",
    "    #print (file)\n",
    "    #print (set(os.listdir(results_csvs)))\n",
    "    #file = file + '.csv'\n",
    "    #if file in set(os.listdir(results_csvs)):\n",
    "    file = file.replace('.csv','')\n",
    "    \n",
    "    with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "            with open(results_csvs_new+file+'.csv', 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Scrapped witnesses')\n",
    "                    all.append(row)\n",
    "                    \n",
    "                    \n",
    "                    for row in reader:\n",
    "                        hit = 0\n",
    "                        if row[7].strip() in scrappedWD.keys(): \n",
    "                            tempWit = scrappedWD[row[7].strip()]\n",
    "                            #print (tempWit)\n",
    "                            name = row[3] +' '+row[5] +' '+ row[17] \n",
    "                            for j in tempWit.split('\\n'):\n",
    "                                if fuzz.token_sort_ratio(\"\".join(j.lower().split()[:4]), name.lower()) > 40 and j.strip()!='':\n",
    "                                    row.append(j.strip())\n",
    "                                    row[16] = 'Yes'\n",
    "                                    hit = 1\n",
    "                                    #break\n",
    "                                    #print (fuzz.token_sort_ratio(\"\".join(j.lower().split()[:4]), name.lower()))\n",
    "                                    #print (\"\".join(j.lower().split()[:4]))\n",
    "                                    #print (name.lower())\n",
    "                                    break\n",
    "                            \n",
    "                            if hit == 0:\n",
    "                                row.append('-')\n",
    "                        else:\n",
    "                            row.append('-')\n",
    "                            \n",
    "                        all.append(row)\n",
    "                    \n",
    "                    writer.writerows(all)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning witness, scrapped witness column\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "for file in set(os.listdir(results_csvs)):\n",
    "    with open(results_csvs+file,'r', encoding=\"utf8\") as csvinput:\n",
    "        with open(results_csvs_new+file, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    all.append(row)\n",
    "                    \n",
    "                    for row in reader:\n",
    "                        \n",
    "                        if row[18].strip() == 'United States Senate' or row[18].strip() == 'United States House of Representatives': \n",
    "                            row[16] = 'No'\n",
    "                            row[20] = '-'\n",
    "                            \n",
    "                        all.append(row)\n",
    "                    \n",
    "                    writer.writerows(all)\n",
    "                    \n",
    "print ('asdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of acronyms and agencies\n",
    " \n",
    "import os\n",
    "\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "gpo2 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019_v2.csv\"\n",
    "\n",
    "\n",
    "#print(df1['filename'])\n",
    "\n",
    "df = pd.read_csv(gpo2)\n",
    "agencies = []\n",
    "acronyms = []\n",
    "\n",
    "acroMap = {}\n",
    "\n",
    "for i in (df['Agency']):    \n",
    "    agencies.append(i)\n",
    "    \n",
    "for i in (df['Alternate Name']):    \n",
    "    acronyms.append(i)\n",
    "    \n",
    "for i in acronyms:\n",
    "    if not(pd.isnull(i)):\n",
    "        index = acronyms.index(i)\n",
    "        acroMap[i] = agencies[index]\n",
    "        \n",
    "print((acroMap.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of acronyms and states\n",
    " \n",
    "import os\n",
    "\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "usstates = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/us_states.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(usstates,  header=None)\n",
    "states= []\n",
    "acronyms = []\n",
    "\n",
    "acroMapStates = {}\n",
    "\n",
    "for i in (df.iloc[:,1]):    \n",
    "    states.append(i)\n",
    "    \n",
    "for i in (df.iloc[:,2]):    \n",
    "    acronyms.append(i)\n",
    "    \n",
    "for i in acronyms:\n",
    "    #if not(pd.isnull(i)):\n",
    "        index = acronyms.index(i)\n",
    "        acroMapStates[i] = states[index]\n",
    "        \n",
    "print((acroMapStates.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPO agencies for individual CSVs\n",
    "# Exact matching on agency names and acronyms, states, Inspector General\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd\n",
    "gpo = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019.csv\"\n",
    "gpo2 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019_v2.csv\"\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "results_csvs_new1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new1/\"\n",
    "\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "sample500 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Sample_500_108th-112th_Congresses_1.31.19.csv\"\n",
    "\n",
    "sample500GPOOutput = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs500-GPOs/\"\n",
    "\n",
    "df1 = pd.read_csv(sample500)\n",
    "#print(df1['filename'])\n",
    "\n",
    "df = pd.read_csv(gpo2)\n",
    "agencies = []\n",
    "for i in (df['Agency']):\n",
    "    temp = i.replace('U.S.', 'United States')\n",
    "    temp = temp.replace('US', 'United States')\n",
    "    temp = temp.replace('Dep.', 'Department')\n",
    "    temp = temp.replace('Dept.', 'Department')\n",
    "    temp = temp.replace('Dept', 'Department')\n",
    "    temp = temp.replace('Assoc', 'Association')\n",
    "    temp = temp.replace('Assoc.', 'Association')\n",
    "    temp = temp.replace('Brd', 'Board')\n",
    "    temp = temp.replace('Brd.', 'Board')\n",
    "    temp = temp.replace('DC', 'District of Columbia')\n",
    "    temp = temp.replace('D.C.', 'District of Columbia')\n",
    "\n",
    "    temp = temp.replace('.,',' ')\n",
    "    temp = temp.replace('.;',' ')\n",
    "    temp = temp.replace('.-',' ')\n",
    "    temp = temp.replace('.:',' ')\n",
    "    temp = temp.replace('.,',' ')\n",
    "                \n",
    "    temp = temp.replace('.', '')\n",
    "    \n",
    "    for i in temp.split():\n",
    "        if i in acroMap.keys():\n",
    "            temp = temp.replace(i,acroMap[i])\n",
    "                    \n",
    "    for i in temp.split():\n",
    "        if i in acroMapStates.keys():\n",
    "            temp = temp.replace(i,acroMapStates[i])\n",
    "    \n",
    "    agencies.append(temp)\n",
    "    \n",
    "JK = []\n",
    "UA = []\n",
    "Parent = []\n",
    "\n",
    "for i in (df['JK Code']):\n",
    "    JK.append(i)\n",
    "for i in (df['UA Code']):\n",
    "    UA.append(i)\n",
    "for i in (df['Parent UA Code']):\n",
    "    Parent.append(i)\n",
    "    \n",
    "    \n",
    "#print (set(agencies))      \n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#file = pd.read_csv(sample_csvs + 'CHRG-105hhrg40051' +'.csv')\n",
    "\n",
    "#for file in df1['filename']:\n",
    "    \n",
    "#    try:\n",
    "        \n",
    "                #print ( row1['Affiliation'] + ' : ' + agency + '\\t' + str(max_score))\n",
    "#agencies = agencies[:100]          \n",
    "for file in set(os.listdir(results_csvs)):\n",
    "    if file not in set(os.listdir(results_csvs_new)):\n",
    "        with open(results_csvs+file,'r', encoding=\"utf8\") as csvinput:\n",
    "            with open(results_csvs_new+file, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                        writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                        reader = csv.reader(csvinput)\n",
    "\n",
    "                        all = []\n",
    "                        row = next(reader)\n",
    "\n",
    "                        row.append('Agency')\n",
    "                        \n",
    "                        \n",
    "                        row.append('JK code')\n",
    "                        row.append('UA code')\n",
    "                        row.append('Parent UA code')\n",
    "                        row.append('US State')\n",
    "                        row.append('Inspector General')\n",
    "                        all.append(row)\n",
    "                        #print (row)\n",
    "                        #try:\n",
    "\n",
    "                        for row in reader:\n",
    "                          \n",
    "                            if row[16] == 'Yes':\n",
    "                                max_score = 0\n",
    "                                agency = '-'\n",
    "                                jk = '-'\n",
    "                                ua = '-'\n",
    "                                parent = '-'\n",
    "                                aff = row[18] +' '+row[20]\n",
    "                                \n",
    "                                aff = aff.replace('U.S.', 'United States')\n",
    "                                aff = aff.replace('US', 'United States')\n",
    "                                aff = aff.replace('Dep.', 'Department')\n",
    "                                aff = aff.replace('Dept.', 'Department')\n",
    "                                aff = aff.replace('Dept', 'Department')\n",
    "                                aff = aff.replace('Assoc', 'Association')\n",
    "                                aff = aff.replace('Assoc.', 'Association')\n",
    "                                aff = aff.replace('Brd', 'Board')\n",
    "                                aff = aff.replace('Brd.', 'Board')\n",
    "                                aff = aff.replace('DC', 'District of Columbia')\n",
    "                                aff = aff.replace('D.C.', 'District of Columbia')\n",
    "                                \n",
    "                                aff = aff.replace('.,',' ')\n",
    "                                aff = aff.replace('.;',' ')\n",
    "                                aff = aff.replace('.-',' ')\n",
    "                                aff = aff.replace('.:',' ')\n",
    "                                aff = aff.replace('.,',' ')\n",
    "                                \n",
    "                                aff = aff.replace('.', '')\n",
    "                                \n",
    "                                for i in aff.split():\n",
    "                                    if i in acroMap.keys():\n",
    "                                        aff = aff.replace(i,acroMap[i])\n",
    "                    \n",
    "                                for i in aff.split():\n",
    "                                    if i in acroMapStates.keys():\n",
    "                                        aff = aff.replace(i,acroMapStates[i])\n",
    "                                  \n",
    "                                hit = 0\n",
    "                                for i in ((agencies)):\n",
    "                                    #score = fuzz.WRatio( i, aff )\n",
    "                                    #if (score > max_score):\n",
    "                                    if i in aff:\n",
    "                                        #max_score = score\n",
    "                                        agency = i\n",
    "                                        index = agencies.index(i)\n",
    "                                        jk = JK[index]\n",
    "                                        ua = UA[index]\n",
    "                                        parent = Parent[index]\n",
    "                                        \n",
    "                                        row.append(agency)\n",
    "                                        row.append(jk)\n",
    "                                        row.append(ua)\n",
    "                                        row.append(parent)\n",
    "                                        hit = 1\n",
    "                                        break\n",
    "                                '''\n",
    "                                if max_score >= 90:\n",
    "                                    row.append(agency)\n",
    "                                    row.append(jk)\n",
    "                                    row.append(ua)\n",
    "                                    row.append(parent)\n",
    "                                    \n",
    "                                else:\n",
    "                                    row.append('-')\n",
    "                                    row.append('-')\n",
    "                                    row.append('-')\n",
    "                                    row.append('-')\n",
    "                                '''\n",
    "                                \n",
    "                                if hit == 0:\n",
    "                                    row.append('-')\n",
    "                                    row.append('-')\n",
    "                                    row.append('-')\n",
    "                                    row.append('-')\n",
    "                                    \n",
    "                                states = 0\n",
    "                                \n",
    "                                for i in acroMapStates.values():\n",
    "                                    if i in aff:\n",
    "                                        row.append(i)\n",
    "                                        states = 1\n",
    "                                        break\n",
    "                                \n",
    "                                if states == 0:\n",
    "                                    row.append('-')\n",
    "                                    \n",
    "                                if 'IG' in aff or 'Inspector General' in aff or 'Inspec. General' in aff:\n",
    "                                    row.append('Yes')\n",
    "                                else:\n",
    "                                    row.append('No')\n",
    "                                    \n",
    "                            else:\n",
    "                                row.append('-')\n",
    "                                row.append('-')\n",
    "                                row.append('-')\n",
    "                                row.append('-')\n",
    "                                row.append('-')\n",
    "                                row.append('-')\n",
    "                                \n",
    "                            all.append(row)\n",
    "\n",
    "                        #except:\n",
    "                        #    writer.writerows(all)\n",
    "                        #    continue\n",
    "                        writer.writerows(all)\n",
    "                \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "print (fuzz.partial_ratio( 'Hon. Peter J. Visclosky, a Representative in Congress from the State of Indiana', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPO agencies for metadata\n",
    "# Exact matching on agency names and acronyms, states, Inspector General\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd\n",
    "gpo = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019.csv\"\n",
    "gpo2 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019_v2.csv\"\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "results_csvs_new1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new1/\"\n",
    "\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "sample500 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Sample_500_108th-112th_Congresses_1.31.19.csv\"\n",
    "\n",
    "sample500GPOOutput = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs500-GPOs/\"\n",
    "\n",
    "df1 = pd.read_csv(sample500)\n",
    "#print(df1['filename'])\n",
    "\n",
    "df = pd.read_csv(gpo2)\n",
    "agencies = []\n",
    "for i in (df['Agency']):\n",
    "    temp = i.replace('U.S.', 'United States')\n",
    "    temp = temp.replace('US', 'United States')\n",
    "    temp = temp.replace('Dep.', 'Department')\n",
    "    temp = temp.replace('Dept.', 'Department')\n",
    "    temp = temp.replace('Dept', 'Department')\n",
    "    temp = temp.replace('Assoc', 'Association')\n",
    "    temp = temp.replace('Assoc.', 'Association')\n",
    "    temp = temp.replace('Brd', 'Board')\n",
    "    temp = temp.replace('Brd.', 'Board')\n",
    "    temp = temp.replace('DC', 'District of Columbia')\n",
    "    temp = temp.replace('D.C.', 'District of Columbia')\n",
    "\n",
    "    temp = temp.replace('.,',' ')\n",
    "    temp = temp.replace('.;',' ')\n",
    "    temp = temp.replace('.-',' ')\n",
    "    temp = temp.replace('.:',' ')\n",
    "    temp = temp.replace('.,',' ')\n",
    "                \n",
    "    temp = temp.replace('.', '')\n",
    "    \n",
    "    for i in temp.split():\n",
    "        if i in acroMap.keys():\n",
    "            temp = temp.replace(i,acroMap[i])\n",
    "                    \n",
    "    for i in temp.split():\n",
    "        if i in acroMapStates.keys():\n",
    "            temp = temp.replace(i,acroMapStates[i])\n",
    "    \n",
    "    agencies.append(temp)\n",
    "    \n",
    "JK = []\n",
    "UA = []\n",
    "Parent = []\n",
    "\n",
    "for i in (df['JK Code']):\n",
    "    JK.append(i)\n",
    "for i in (df['UA Code']):\n",
    "    UA.append(i)\n",
    "for i in (df['Parent UA Code']):\n",
    "    Parent.append(i)\n",
    "    \n",
    "    \n",
    "#print (set(agencies))      \n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#file = pd.read_csv(sample_csvs + 'CHRG-105hhrg40051' +'.csv')\n",
    "\n",
    "#for file in df1['filename']:\n",
    "    \n",
    "#    try:\n",
    "        \n",
    "                #print ( row1['Affiliation'] + ' : ' + agency + '\\t' + str(max_score))\n",
    "#agencies = agencies[:100]          \n",
    "#for file in set(os.listdir(results_csvs)):\n",
    "#    if file not in set(os.listdir(results_csvs_new)):\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "            with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                        writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                        reader = csv.reader(csvinput)\n",
    "\n",
    "                        all = []\n",
    "                        row = next(reader)\n",
    "\n",
    "                        row.append('Agency')\n",
    "                        \n",
    "                        \n",
    "                        row.append('JK code')\n",
    "                        row.append('UA code')\n",
    "                        row.append('Parent UA code')\n",
    "                        row.append('US State')\n",
    "                        row.append('Inspector General')\n",
    "                        all.append(row)\n",
    "                        #print (row)\n",
    "                        #try:\n",
    "\n",
    "                        for row in reader:\n",
    "                          \n",
    "                            if row[13].strip() != '-':\n",
    "                                max_score = 0\n",
    "                                agency = '-'\n",
    "                                jk = '-'\n",
    "                                ua = '-'\n",
    "                                parent = '-'\n",
    "                                \n",
    "                                agencyL = []\n",
    "                                jkL = []\n",
    "                                uaL = []\n",
    "                                parentL = []\n",
    "                                stateL = []\n",
    "                                IGL = []\n",
    "                                \n",
    "                                if row[13] == 'Refer column R':\n",
    "                                    affs = row[17].split('\\n')\n",
    "                                else:\n",
    "                                    affs = row[13].split('\\n')\n",
    "                                  \n",
    "                                for aff in affs:\n",
    "                                    if aff.strip() != '':\n",
    "                                        aff = aff.replace('U.S.', 'United States')\n",
    "                                        aff = aff.replace('US', 'United States')\n",
    "                                        aff = aff.replace('Dep.', 'Department')\n",
    "                                        aff = aff.replace('Dept.', 'Department')\n",
    "                                        aff = aff.replace('Dept', 'Department')\n",
    "                                        aff = aff.replace('Assoc', 'Association')\n",
    "                                        aff = aff.replace('Assoc.', 'Association')\n",
    "                                        aff = aff.replace('Brd', 'Board')\n",
    "                                        aff = aff.replace('Brd.', 'Board')\n",
    "                                        aff = aff.replace('DC', 'District of Columbia')\n",
    "                                        aff = aff.replace('D.C.', 'District of Columbia')\n",
    "\n",
    "                                        aff = aff.replace('.,',' ')\n",
    "                                        aff = aff.replace('.;',' ')\n",
    "                                        aff = aff.replace('.-',' ')\n",
    "                                        aff = aff.replace('.:',' ')\n",
    "                                        aff = aff.replace('.,',' ')\n",
    "\n",
    "                                        aff = aff.replace('.', '')\n",
    "\n",
    "                                        for i in aff.split():\n",
    "                                            if i in acroMap.keys():\n",
    "                                                aff = aff.replace(i,acroMap[i])\n",
    "\n",
    "                                        for i in aff.split():\n",
    "                                            if i in acroMapStates.keys():\n",
    "                                                aff = aff.replace(i,acroMapStates[i])\n",
    "\n",
    "                                        hit = 0\n",
    "                                        for i in ((agencies)):\n",
    "                                            #score = fuzz.WRatio( i, aff )\n",
    "                                            #if (score > max_score):\n",
    "                                            if i in aff:\n",
    "                                                #max_score = score\n",
    "                                                agency = i\n",
    "                                                index = agencies.index(i)\n",
    "                                                jk = JK[index]\n",
    "                                                ua = UA[index]\n",
    "                                                parent = Parent[index]\n",
    "\n",
    "                                                agencyL.append(str(agency))\n",
    "                                                jkL.append(str(jk))\n",
    "                                                uaL.append(str(ua))\n",
    "                                                parentL.append(str(parent))\n",
    "                                                hit = 1\n",
    "                                                break\n",
    "                                        '''\n",
    "                                        if max_score >= 90:\n",
    "                                            row.append(agency)\n",
    "                                            row.append(jk)\n",
    "                                            row.append(ua)\n",
    "                                            row.append(parent)\n",
    "\n",
    "                                        else:\n",
    "                                            row.append('-')\n",
    "                                            row.append('-')\n",
    "                                            row.append('-')\n",
    "                                            row.append('-')\n",
    "                                        '''\n",
    "\n",
    "                                        if hit == 0:\n",
    "                                            agencyL.append('-')\n",
    "                                            jkL.append('-')\n",
    "                                            uaL.append('-')\n",
    "                                            parentL.append('-')\n",
    "\n",
    "                                        states = 0\n",
    "\n",
    "                                        for i in acroMapStates.values():\n",
    "                                            if i in aff:\n",
    "                                                stateL.append(i)\n",
    "                                                states = 1\n",
    "                                                break\n",
    "\n",
    "                                        if states == 0:\n",
    "                                            stateL.append('-')\n",
    "\n",
    "                                        if 'IG' in aff or 'Inspector General' in aff or 'Inspec. General' in aff:\n",
    "                                            IGL.append('Yes')\n",
    "                                        else:\n",
    "                                            IGL.append('No')\n",
    "                                \n",
    "                                row.append(\"\\n\".join(agencyL))\n",
    "                                row.append(\"\\n\".join(jkL))\n",
    "                                row.append(\"\\n\".join(uaL))\n",
    "                                row.append(\"\\n\".join(parentL))\n",
    "                                row.append(\"\\n\".join(stateL))\n",
    "                                row.append(\"\\n\".join(IGL))\n",
    "                                \n",
    "                            else:\n",
    "                                row.append('-')\n",
    "                                row.append('-')\n",
    "                                row.append('-')\n",
    "                                row.append('-')\n",
    "                                row.append('-')\n",
    "                                row.append('-')\n",
    "                                \n",
    "                            all.append(row)\n",
    "\n",
    "                        #except:\n",
    "                        #    writer.writerows(all)\n",
    "                        #    continue\n",
    "                        writer.writerows(all)\n",
    "                \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding \"Bills\" column in all individual CSVs\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "FullTexts = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/FullTexts/\"\n",
    "\n",
    "\n",
    "#df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = ['CHRG-115hhrg27211']\n",
    "count = 0\n",
    "\n",
    "#files = set(os.listdir(results_csvs)) - set(os.listdir(results_csvs_new))\n",
    "\n",
    "countWitness = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gpo = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019.csv\"\n",
    "gpo2 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Master agencies list_Feb. 2019_v2.csv\"\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "sample500 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/Sample_500_108th-112th_Congresses_1.31.19.csv\"\n",
    "\n",
    "sample500GPOOutput = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs500-GPOs/\"\n",
    "\n",
    "sample500SAOutput = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs500-SA/\"\n",
    "\n",
    "sampleBill = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/CHRG-109shrg26254_coded_bills (1).csv\"\n",
    "\n",
    "#df1 = pd.read_csv(sample500)\n",
    "#print(df1['filename'])\n",
    "\n",
    "#sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "#print (set(agencies))      \n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#file = pd.read_csv(sample_csvs + 'CHRG-105hhrg40051' +'.csv')\n",
    "count = 0\n",
    "#for file in set(os.listdir(results_csvs)):\n",
    "for file in set(os.listdir(results_csvs)):\n",
    "    #print (file)\n",
    "    #print (set(os.listdir(results_csvs)))\n",
    "    #file = file + '.csv'\n",
    "    #if file in set(os.listdir(results_csvs)):\n",
    "    file = file.replace('.csv','')\n",
    "    \n",
    "    \n",
    "    with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "            with open(results_csvs_new+file+'.csv', 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Bills')\n",
    "                    all.append(row)\n",
    "                    \n",
    "                    \n",
    "                    for row in reader:\n",
    "                        if re.search(r\"(S\\.\\d{4})\",row[12]) or re.search(r\"(S\\. \\d{4})\",row[12]) or re.search(r\"(S\\d{4})\",row[12]) or re.search(r\"(S \\d{4})\",row[12]) or re.search(r\"(H\\.R\\.\\. \\d{4})\",row[12]) or re.search(r\"(HR \\d{4})\",row[12]) or re.search(r\"(H\\.R\\.\\d{4})\",row[12]) or re.search(r\"(HR\\d{4})\",row[12]):\n",
    "                        #if re.search(r\"(.)*(S\\.\\d{4})*(S\\. \\d{4})*(S\\d{4})*(S \\d{4})*(H\\.R\\.\\. \\d{4})*(HR \\d{4})*(H\\.R\\.\\d{4})*(HR\\d{4})*(.)*$\",row[12]):\n",
    "                            row.append('1')\n",
    "                            count += 1\n",
    "                           # print(count)\n",
    "                        else:\n",
    "                            row.append('0')\n",
    "                            \n",
    "                        all.append(row)\n",
    "                        #break\n",
    "                    writer.writerows(all)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"No. of bills found : \")\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "years = ['1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012']      \n",
    "committees = [102, 104, 106, 113, 115, 124, 128, 134, 138, 142, 156, 164, 173, 176, 182, 184, 186, 192, 196, 242, 251, 305, 306, 308, 314, 316, 321, 330, 332, 336, 338, 344, 358, 362, 380, 384, 388, 419, 432, 434, 435]\n",
    "congresses = [104, 105, 106, 107, 108, 109, 110, 111, 112]\n",
    "\n",
    "gpoShort = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/ChenJohnson_Agencies.csv\"\n",
    "\n",
    "df = pd.read_csv(gpoShort)\n",
    "agencies = []\n",
    "\n",
    "for i in (df['Agency']):    \n",
    "    agencies.append(i)\n",
    "    \n",
    "JK = []\n",
    "UA = []\n",
    "Parent = []\n",
    "\n",
    "for i in (df['JK Code']):\n",
    "    JK.append(i)\n",
    "for i in (df['UA Code']):\n",
    "    UA.append(i)\n",
    "for i in (df['Parent UA Code']):\n",
    "    Parent.append(i)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 1: Number of utterances made by the agency about a bill per month\n",
    "\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV11.csv\"\n",
    "\n",
    "#for file in set(os.listdir(results_csvs)):\n",
    "    #print (file)\n",
    "    #print (set(os.listdir(results_csvs)))\n",
    "    #file = file + '.csv'\n",
    "    #if file in set(os.listdir(results_csvs)):\n",
    "#file = file.replace('.csv','')\n",
    "    \n",
    "    \n",
    "#with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    writer.writerow([\"Date\", \"Committee\", \"Agency\", \"JK Code\", \"UA Code\", \"Parent UA Code\"])\n",
    "  \n",
    "                    for committee in committees:\n",
    "                        for month in months:\n",
    "                            for year in years:\n",
    "                                for i in range(len(agencies)):\n",
    "                                    row_temp = \"=\\\"\" +month+'-'+year+\"\\\"\", committee, agencies[i], JK[i], UA[i], Parent[i]\n",
    "                                    writer.writerow(row_temp)\n",
    "                                    \n",
    "                                    \n",
    "            \n",
    "                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove duplicate ent\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV11.csv\"\n",
    "from more_itertools import unique_everseen\n",
    "with open(CSV1,'r') as f, open('2.csv','w') as out_file:\n",
    "    out_file.writelines(unique_everseen(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.values())[0:100000])\n",
    "print(CSV1Dict['06-1998 344 United States Postal Service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 1: Number of utterances made by the agency about a bill per month\n",
    "\n",
    "CSV221 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV221.csv\"\n",
    "\n",
    "monthDict ={'01':'Jan',\n",
    "           '02':'Feb',\n",
    "            '03':'Mar',\n",
    "            '04':'Apr',\n",
    "            '05':'May',\n",
    "            '06':'Jun',\n",
    "            '07':'Jul',\n",
    "            '08':'Aug',\n",
    "            '09':'Sep',\n",
    "            '10':'Oct',\n",
    "            '11':'Nov',\n",
    "            '12':'Dec'\n",
    "            \n",
    "           }\n",
    "with open(CSV1,'r', encoding=\"utf8\") as csvinput:\n",
    "            with open(CSV221, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Number of utterances made by the agency about a bill per month')\n",
    "                    all.append(row)\n",
    "                    \n",
    "                    for row in reader:\n",
    "                        CSV1RowDate = str(row[0])\n",
    "                        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "                        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "\n",
    "                        CSV1key =  CSV1RowDate+' '+ row[1] +' '+ row[2].strip()\n",
    "                        \n",
    "                        if CSV1key in CSV1Dict.keys():\n",
    "                            row.append(CSV1Dict[CSV1key])\n",
    "                            \n",
    "                        all.append(row)\n",
    "                        #break\n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of utterances made by the agency per month - CSV2\n",
    "\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV1.csv\"\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "\n",
    "CSV1Dict = {}\n",
    "\n",
    "utteranceCount = []\n",
    "\n",
    "with open(CSV1, 'r', encoding=\"utf8\") as csvinput2:\n",
    "    CSV1reader = csv.reader(csvinput2)\n",
    "    \n",
    "    #all = []\n",
    "    CSV1row = next(CSV1reader)\n",
    "\n",
    "    #CSV1row.append('Number of utterances made by the agency about a bill per month')\n",
    "    #all.append(CSV1row)\n",
    "                \n",
    "    for CSV1row in CSV1reader:\n",
    "        count = 0\n",
    "        \n",
    "        CSV1RowDate = str(CSV1row[0])\n",
    "        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "                \n",
    "        CSV1key =  CSV1RowDate+' '+ CSV1row[1] +' '+ CSV1row[2].strip() \n",
    "        \n",
    "        CSV1Dict[CSV1key.strip()] = 0\n",
    "\n",
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.keys())[0])\n",
    "\n",
    "for file in set(os.listdir(results_csvs)):\n",
    "            \n",
    "            file = file.replace('.csv','')\n",
    "\n",
    "            with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                #if row[27] == '1':\n",
    "                                    \n",
    "                                    date = row[13].split('-')[0]+'-'+row[13].split('-')[2]\n",
    "                                    indCSVkey = date +' '+ row[0] + ' '+ row[21].strip()\n",
    "                                    \n",
    "                                    #print(indCSVkey)\n",
    "                                    \n",
    "                                    if indCSVkey.strip() in CSV1Dict.keys():\n",
    "                                        CSV1Dict[indCSVkey.strip()] += 1\n",
    "                                        #print(indCSVkey)\n",
    "                                        \n",
    "        #print(count)\n",
    "        #utteranceCount.append(count)\n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of utterances made by the agency per month - CSV2\n",
    "\n",
    "CSV211 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV211.csv\"\n",
    "\n",
    "monthDict ={'01':'Jan',\n",
    "           '02':'Feb',\n",
    "            '03':'Mar',\n",
    "            '04':'Apr',\n",
    "            '05':'May',\n",
    "            '06':'Jun',\n",
    "            '07':'Jul',\n",
    "            '08':'Aug',\n",
    "            '09':'Sep',\n",
    "            '10':'Oct',\n",
    "            '11':'Nov',\n",
    "            '12':'Dec'\n",
    "            \n",
    "           }\n",
    "\n",
    "with open(CSV1,'r', encoding=\"utf8\") as csvinput:\n",
    "            with open(CSV211, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Number of utterances made by the agency per month')\n",
    "                    all.append(row)\n",
    "                    \n",
    "                    for row in reader:\n",
    "                        CSV1RowDate = str(row[0])\n",
    "                        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "                        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "\n",
    "                        CSV1key =  CSV1RowDate+' '+ row[1] +' '+ row[2].strip()\n",
    "                        \n",
    "                        if CSV1key in CSV1Dict.keys():\n",
    "                            row.append(CSV1Dict[CSV1key])\n",
    "                            \n",
    "                        all.append(row)\n",
    "                        #break\n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each committee, need the number of total utterances per month - CSV3\n",
    "\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV1.csv\"\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "\n",
    "CSV1Dict = {}\n",
    "\n",
    "utteranceCount = []\n",
    "\n",
    "with open(CSV1, 'r', encoding=\"utf8\") as csvinput2:\n",
    "    CSV1reader = csv.reader(csvinput2)\n",
    "    \n",
    "    #all = []\n",
    "    CSV1row = next(CSV1reader)\n",
    "\n",
    "    #CSV1row.append('Number of utterances made by the agency about a bill per month')\n",
    "    #all.append(CSV1row)\n",
    "                \n",
    "    for CSV1row in CSV1reader:\n",
    "        count = 0\n",
    "        \n",
    "        CSV1RowDate = str(CSV1row[0])\n",
    "        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "                \n",
    "        CSV1key =  CSV1RowDate+' '+ CSV1row[1] #+' '+ CSV1row[2].strip() \n",
    "        \n",
    "        CSV1Dict[CSV1key.strip()] = 0\n",
    "\n",
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.keys())[0])\n",
    "\n",
    "for file in set(os.listdir(results_csvs)):\n",
    "            \n",
    "            file = file.replace('.csv','')\n",
    "\n",
    "            with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                #if row[27] == '1':\n",
    "                                    \n",
    "                                    date = row[13].split('-')[0]+'-'+row[13].split('-')[2]\n",
    "                                    indCSVkey = date +' '+ row[0] #+ ' '+ row[21].strip()\n",
    "                                    \n",
    "                                    #print(indCSVkey)\n",
    "                                    \n",
    "                                    if indCSVkey.strip() in CSV1Dict.keys():\n",
    "                                        CSV1Dict[indCSVkey.strip()] += 1\n",
    "                                        #print(indCSVkey)\n",
    "                                        \n",
    "        #print(count)\n",
    "        #utteranceCount.append(count)\n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each committee, need the number of total utterances per month - CSV3\n",
    "\n",
    "\n",
    "CSV2111 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV2111.csv\"\n",
    "\n",
    "monthDict ={'01':'Jan',\n",
    "           '02':'Feb',\n",
    "            '03':'Mar',\n",
    "            '04':'Apr',\n",
    "            '05':'May',\n",
    "            '06':'Jun',\n",
    "            '07':'Jul',\n",
    "            '08':'Aug',\n",
    "            '09':'Sep',\n",
    "            '10':'Oct',\n",
    "            '11':'Nov',\n",
    "            '12':'Dec'\n",
    "            \n",
    "           }\n",
    "\n",
    "with open(CSV1,'r', encoding=\"utf8\") as csvinput:\n",
    "            with open(CSV2111, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Number of utterances made by the committees per month')\n",
    "                    all.append(row)\n",
    "                    \n",
    "                    for row in reader:\n",
    "                        CSV1RowDate = str(row[0])\n",
    "                        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "                        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "\n",
    "                        CSV1key =  CSV1RowDate+' '+ row[1] #+' '+ row[2].strip()\n",
    "                        \n",
    "                        if CSV1key in CSV1Dict.keys():\n",
    "                            row.append(CSV1Dict[CSV1key])\n",
    "                            \n",
    "                        all.append(row)\n",
    "                        #break\n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each agency, need the number of total utterances per month - CSV4\n",
    "\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV1.csv\"\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "\n",
    "CSV1Dict = {}\n",
    "\n",
    "utteranceCount = []\n",
    "\n",
    "with open(CSV1, 'r', encoding=\"utf8\") as csvinput2:\n",
    "    CSV1reader = csv.reader(csvinput2)\n",
    "    \n",
    "    #all = []\n",
    "    CSV1row = next(CSV1reader)\n",
    "\n",
    "    #CSV1row.append('Number of utterances made by the agency about a bill per month')\n",
    "    #all.append(CSV1row)\n",
    "                \n",
    "    for CSV1row in CSV1reader:\n",
    "        count = 0\n",
    "        \n",
    "        CSV1RowDate = str(CSV1row[0])\n",
    "        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "                \n",
    "        CSV1key =  CSV1RowDate+' '+ CSV1row[1].strip() \n",
    "        \n",
    "        CSV1Dict[CSV1key.strip()] = 0\n",
    "\n",
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.keys())[0])\n",
    "\n",
    "for file in set(os.listdir(results_csvs)):\n",
    "            \n",
    "            file = file.replace('.csv','')\n",
    "\n",
    "            with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                #if row[27] == '1':\n",
    "                                    \n",
    "                                    date = row[13].split('-')[0]+'-'+row[13].split('-')[2]\n",
    "                                    indCSVkey = date + ' '+ row[21].strip()\n",
    "                                    \n",
    "                                    #print(indCSVkey)\n",
    "                                    \n",
    "                                    if indCSVkey.strip() in CSV1Dict.keys():\n",
    "                                        CSV1Dict[indCSVkey.strip()] += 1\n",
    "                                        #print(indCSVkey)\n",
    "                                        \n",
    "        #print(count)\n",
    "        #utteranceCount.append(count)\n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each agency, need the number of total utterances per month - CSV4\n",
    "\n",
    "\n",
    "\n",
    "CSV4 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV4.csv\"\n",
    "\n",
    "monthDict ={'01':'Jan',\n",
    "           '02':'Feb',\n",
    "            '03':'Mar',\n",
    "            '04':'Apr',\n",
    "            '05':'May',\n",
    "            '06':'Jun',\n",
    "            '07':'Jul',\n",
    "            '08':'Aug',\n",
    "            '09':'Sep',\n",
    "            '10':'Oct',\n",
    "            '11':'Nov',\n",
    "            '12':'Dec'\n",
    "            \n",
    "           }\n",
    "\n",
    "with open(CSV1,'r', encoding=\"utf8\") as csvinput:\n",
    "            with open(CSV4, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Number of utterances made by the agencies per month')\n",
    "                    all.append(row)\n",
    "                    \n",
    "                    for row in reader:\n",
    "                        CSV1RowDate = str(row[0])\n",
    "                        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "                        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "\n",
    "                        CSV1key =  CSV1RowDate+' '+ row[1].strip()\n",
    "                        \n",
    "                        if CSV1key in CSV1Dict.keys():\n",
    "                            row.append(CSV1Dict[CSV1key])\n",
    "                            \n",
    "                        all.append(row)\n",
    "                        #break\n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hearings made by the agency per month - CSV5\n",
    "\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV1.csv\"\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "\n",
    "CSV1Dict = {}\n",
    "\n",
    "utteranceCount = []\n",
    "\n",
    "with open(CSV1, 'r', encoding=\"utf8\") as csvinput2:\n",
    "    CSV1reader = csv.reader(csvinput2)\n",
    "    \n",
    "    #all = []\n",
    "    CSV1row = next(CSV1reader)\n",
    "\n",
    "    #CSV1row.append('Number of utterances made by the agency about a bill per month')\n",
    "    #all.append(CSV1row)\n",
    "                \n",
    "    for CSV1row in CSV1reader:\n",
    "        count = 0\n",
    "        \n",
    "        CSV1RowDate = str(CSV1row[0])\n",
    "        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "                \n",
    "        CSV1key =  CSV1RowDate+' '+ CSV1row[1] +' '+ CSV1row[2].strip() \n",
    "        \n",
    "        CSV1Dict[CSV1key.strip()] = 0\n",
    "\n",
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.keys())[0])\n",
    "\n",
    "\n",
    "hearingsSet = set()\n",
    "for file in set(os.listdir(results_csvs)):\n",
    "            \n",
    "            file = file.replace('.csv','')\n",
    "            \n",
    "            hearingsSet.clear()\n",
    "            with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                #if row[27] == '1':\n",
    "                                    \n",
    "                                    date = row[13].split('-')[0]+'-'+row[13].split('-')[2]\n",
    "                                    indCSVkey = date +' '+ row[0] + ' '+ row[21].strip()\n",
    "                                    \n",
    "                                    hearingsSet.add(indCSVkey)\n",
    "                                    #print(indCSVkey)\n",
    "                                    \n",
    "            for i in hearingsSet:\n",
    "                if i.strip() in CSV1Dict.keys():\n",
    "                    CSV1Dict[i.strip()] += 1\n",
    "                                        #print(indCSVkey)\n",
    "                                        \n",
    "        #print(count)\n",
    "        #utteranceCount.append(count)\n",
    "            \n",
    "\n",
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.values())[0])              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hearings made by the agency per month - CSV5\n",
    "\n",
    "CSV5 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV5.csv\"\n",
    "\n",
    "monthDict ={'01':'Jan',\n",
    "           '02':'Feb',\n",
    "            '03':'Mar',\n",
    "            '04':'Apr',\n",
    "            '05':'May',\n",
    "            '06':'Jun',\n",
    "            '07':'Jul',\n",
    "            '08':'Aug',\n",
    "            '09':'Sep',\n",
    "            '10':'Oct',\n",
    "            '11':'Nov',\n",
    "            '12':'Dec'\n",
    "            \n",
    "           }\n",
    "\n",
    "with open(CSV1,'r', encoding=\"utf8\") as csvinput:\n",
    "            with open(CSV5, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Number of hearings made by the agency per month')\n",
    "                    all.append(row)\n",
    "                    \n",
    "                    for row in reader:\n",
    "                        CSV1RowDate = str(row[0])\n",
    "                        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "                        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "\n",
    "                        CSV1key =  CSV1RowDate+' '+ row[1] +' '+ row[2].strip()\n",
    "                        \n",
    "                        if CSV1key in CSV1Dict.keys():\n",
    "                            row.append(CSV1Dict[CSV1key])\n",
    "                            \n",
    "                        all.append(row)\n",
    "                        #break\n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding gender based on names\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV1.csv\"\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "\n",
    "namesDict = {}\n",
    "\n",
    "for file in set(os.listdir(results_csvs)):\n",
    "            \n",
    "            file = file.replace('.csv','')\n",
    "            \n",
    "            hearingsSet.clear()\n",
    "            with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                if row[17].strip() != 'NA' or row[17].strip() != '-':\n",
    "                                    namesDict[row[17]] = 'M/F'\n",
    "                                    #print(indCSVkey)\n",
    "         \n",
    "            \n",
    "\n",
    "print(len(namesDict.keys()))\n",
    "print(list(namesDict.values())[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding gender based on names\n",
    "\n",
    "import gender_guesser.detector as gender\n",
    "d = gender.Detector()\n",
    "\n",
    "for i in range(30):\n",
    "    print((list(namesDict.keys())[i]) + \" : \"+ d.get_gender(list(namesDict.keys())[i]))\n",
    "    #print('\\n')\n",
    "\n",
    "print(d.get_gender(u\"Mainzer\"))\n",
    "print(d.get_gender(u\"Bob\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding gender based on names\n",
    "\n",
    "import random \n",
    "from nltk.corpus import names \n",
    "import nltk \n",
    "  \n",
    "def gender_features(word): \n",
    "    return {'last_letter':word[-1]} \n",
    "  \n",
    "# preparing a list of examples and corresponding class labels. \n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')]+\n",
    "             [(name, 'female') for name in names.words('female.txt')]) \n",
    "  \n",
    "random.shuffle(labeled_names) \n",
    "  \n",
    "# we use the feature extractor to process the names data. \n",
    "featuresets = [(gender_features(n), gender)  \n",
    "               for (n, gender)in labeled_names] \n",
    "  \n",
    "# Divide the resulting list of feature \n",
    "# sets into a training set and a test set. \n",
    "train_set, test_set = featuresets[5:], featuresets[:5] \n",
    "  \n",
    "# The training set is used to  \n",
    "# train a new \"naive Bayes\" classifier. \n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) \n",
    "  \n",
    "\n",
    "for i in range(30):\n",
    "    print((list(namesDict.keys())[i]) + \" : \"+ classifier.classify(gender_features((list(namesDict.keys())[i]))))\n",
    "    #print('\\n')\n",
    "\n",
    "print(classifier.classify(gender_features('Bob')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(namesDict.keys()))\n",
    "print(list(namesDict.values())[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = \"namesDict.txt\"\n",
    "fo = open(fout, \"w\")\n",
    "\n",
    "for k, v in namesDict.items():\n",
    "    fo.write(str(k) +'\\n')\n",
    "\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for k, v in namesDict.items():\n",
    "    if (str(k).find(',')!=-1 ):\n",
    "        count += 1\n",
    "        \n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "# subCommittee extraction\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = ['CHRG-115hhrg27211']\n",
    "count = 0\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('subCommittee')\n",
    "                    all.append(row)\n",
    "                 \n",
    "\n",
    "                    for row in reader:\n",
    "                        try:\n",
    "                            \n",
    "                            file = row[6] + \".json\"\n",
    "                            \n",
    "                            with open(APIs+file) as data_file:    \n",
    "                                jsonObj = json.load(data_file)\n",
    "                            \n",
    "                            \n",
    "\n",
    "                                if (jsonObj[\"mods\"][\"extension\"][2][\"congCommittee\"][\"subCommittee\"][\"name\"][\"#text\"]):\n",
    "                                    subCommittee = jsonObj[\"mods\"][\"extension\"][2][\"congCommittee\"][\"subCommittee\"][\"name\"][\"#text\"]\n",
    "                                    row.append(subCommittee)\n",
    "                                    #print (subCommittee)\n",
    "                                else:\n",
    "                                    row.append('-')\n",
    "             \n",
    "                        except:\n",
    "                            row.append(\"-\")\n",
    "\n",
    "                        all.append(row)\n",
    "\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "# Column: \"Committee member count\"\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "#print(df1['filename'])\n",
    "\n",
    "sample_jackets = ['CHRG-115hhrg27211']\n",
    "count = 0\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Committee member count')\n",
    "                    all.append(row)\n",
    "                 \n",
    "\n",
    "                    for row in reader:\n",
    "                            count = len(row[14].split('\\n'))\n",
    "                            \n",
    "                            row.append(count)\n",
    "                    \n",
    "                            all.append(row)\n",
    "\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "# Column: \"Denominator count\"\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "House = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/house_assignments_103-115-3.csv\"\n",
    "Senate = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/senate_assignments_103-115-3.csv\"\n",
    "\n",
    "CongCom = {}\n",
    "\n",
    "with open(House,'r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                if(row[0]+':'+row[1] in CongCom.keys()):\n",
    "                                    CongCom[row[0]+':'+row[1]] += 1\n",
    "                                else:\n",
    "                                    CongCom[row[0]+':'+row[1]] = 1\n",
    "                                    \n",
    "with open(Senate,'r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                if(row[0]+':'+row[1] in CongCom.keys()):\n",
    "                                    CongCom[row[0]+':'+row[1]] += 1\n",
    "                                else:\n",
    "                                    CongCom[row[0]+':'+row[1]] = 1\n",
    "\n",
    "\n",
    "#print(CongCom)\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Denominator count')\n",
    "                    all.append(row)\n",
    "                 \n",
    "\n",
    "                    for row in reader:\n",
    "                            if (row[2].replace(\"th\",\"\")+':'+row[3]) in CongCom.keys():\n",
    "                                count = CongCom[(row[2].replace(\"th\",\"\")+':'+row[3])]\n",
    "                            else:\n",
    "                                count = '-'\n",
    "                                \n",
    "                            row.append(count)\n",
    "                            all.append(row)\n",
    "\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "# Column: \"Party count\"\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "House = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/house_assignments_103-115-3.csv\"\n",
    "Senate = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/senate_assignments_103-115-3.csv\"\n",
    "\n",
    "CongCom = {}\n",
    "\n",
    "with open(House,'r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                if(row[0]+':'+row[1]+':'+row[6] in CongCom.keys()):\n",
    "                                    CongCom[row[0]+':'+row[1]+':'+row[6]] += 1\n",
    "                                else:\n",
    "                                    CongCom[row[0]+':'+row[1]+':'+row[6]] = 1\n",
    "                                    \n",
    "with open(Senate,'r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                if(row[0]+':'+row[1]+':'+row[6] in CongCom.keys()):\n",
    "                                    CongCom[row[0]+':'+row[1]+':'+row[6]] += 1\n",
    "                                else:\n",
    "                                    CongCom[row[0]+':'+row[1]+':'+row[6]] = 1\n",
    "\n",
    "\n",
    "#print(CongCom)\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Party count(100:200:328:999:9999)')\n",
    "                    all.append(row)\n",
    "                 \n",
    "\n",
    "                    for row in reader:\n",
    "                            if (row[2].replace(\"th\",\"\")+':'+row[3]+':100') in CongCom.keys():\n",
    "                                count100 = CongCom[(row[2].replace(\"th\",\"\")+':'+row[3]+':100')]\n",
    "                            else:\n",
    "                                count100 = '-'\n",
    "                                \n",
    "                            if (row[2].replace(\"th\",\"\")+':'+row[3]+':200') in CongCom.keys():\n",
    "                                count200 = CongCom[(row[2].replace(\"th\",\"\")+':'+row[3]+':200')]\n",
    "                            else:\n",
    "                                count200 = '-'\n",
    "                                \n",
    "                            if (row[2].replace(\"th\",\"\")+':'+row[3]+':328') in CongCom.keys():\n",
    "                                count328 = CongCom[(row[2].replace(\"th\",\"\")+':'+row[3]+':328')]\n",
    "                            else:\n",
    "                                count328 = '-'\n",
    "                                \n",
    "                            if (row[2].replace(\"th\",\"\")+':'+row[3]+':999') in CongCom.keys():\n",
    "                                count999 = CongCom[(row[2].replace(\"th\",\"\")+':'+row[3]+':999')]\n",
    "                            else:\n",
    "                                count999 = '-'\n",
    "                            \n",
    "                            if (row[2].replace(\"th\",\"\")+':'+row[3]+':9999') in CongCom.keys():\n",
    "                                count9999 = CongCom[(row[2].replace(\"th\",\"\")+':'+row[3]+':9999')]\n",
    "                            else:\n",
    "                                count9999 = '-'\n",
    "                                \n",
    "                            temp = \"=\\\"\" + str(count100) + \":\" + str(count200) +  \":\" + str(count328) + \":\" + str(count999) + \":\" + str(count9999) + \"\\\"\"\n",
    "                            \n",
    "                            row.append( temp)\n",
    "                            all.append(row)\n",
    "\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding unique party codes\n",
    "PartyCodes = {}\n",
    "\n",
    "with open(House,'r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                if(row[6] in PartyCodes.keys()):\n",
    "                                    PartyCodes[row[6]] += 1\n",
    "                                else:\n",
    "                                    PartyCodes[row[6]] = 0\n",
    "                                    \n",
    "print(PartyCodes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "# Column: \"Party & Committee info:\"\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "House = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/house_assignments_103-115-3.csv\"\n",
    "Senate = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/senate_assignments_103-115-3.csv\"\n",
    "\n",
    "PartyCom = {}\n",
    "\n",
    "with open(House,'r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                PartyCom[row[0]+row[1]+row[3].lower().strip()] = row[6]+':'+row[9]+':'+row[10]\n",
    "                                \n",
    "                                    \n",
    "with open(Senate,'r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                PartyCom[row[0]+row[1]+row[3].lower().strip()] = row[6]+':'+row[10]+':'+row[11]\n",
    "\n",
    "\n",
    "#print(CongCom)\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('Party & Committee info(Party:Senior Party Member:Committee Seniority')\n",
    "                    all.append(row)\n",
    "                 \n",
    "                    for row in reader:\n",
    "                            temp = []\n",
    "                            for name in row[14].split('\\n'):\n",
    "                                if row[2].replace(\"th\",\"\")+row[3]+name.split(' : ')[0].lower().strip().replace(';','') in PartyCom.keys():\n",
    "                                    temp.append(  PartyCom[row[2].replace(\"th\",\"\")+row[3]+name.split(' : ')[0].lower().strip().replace(';','')] )\n",
    "                                else:\n",
    "                                    temp.append( '-'+'-'+'-')\n",
    "                                    \n",
    "                            row.append(\"\\n\".join(temp))\n",
    "                            all.append(row)\n",
    "\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "# Column: \"Expertise\"\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "House = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/house_assignments_103-115-3.csv\"\n",
    "Senate = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/senate_assignments_103-115-3.csv\"\n",
    "\n",
    "expertise={\n",
    "    \n",
    "    'A.A.' : 'Associate of Arts',\n",
    "\t\t'A.S.' : 'Associate of Science', \n",
    "\t\t'A.A.S.' : 'Associate of Applied Science',\n",
    "\t\t'ADN' : 'Associates Degree in Nursing',\n",
    "\t\t'B.A.' : 'Bachelor of Arts', \n",
    "\t\t'B.S.' : 'Bachelor of Science', \n",
    "\t\t'B.E.' : 'Bachelor of Engineering',\n",
    "\t\t'M.A.' : 'Master of Arts', \n",
    "\t\t'M.S.' : 'Master of Science', \n",
    "\t\t'MBA' : 'Master of Business Administration', \n",
    "\t\t'M.Ed.' : 'Master of Education',\n",
    "\t\t'Ph.D.' : 'Doctor of Philosophy', \n",
    "\t\t'DNP' : 'Doctor of Nursing Practice', \n",
    "\t\t'Ed.D.' : 'Doctor of Education', \n",
    "\t     'J.D.' : 'Juris Doctorate, a law degree',\n",
    "\t\t'M.D.' : 'Medical Doctor, a physicians degree',\n",
    "\t\t'D.D.S.' : 'Doctor of Dental Surgery, a dentistry degree',\n",
    "'Pharm.D.' : 'Doctor of Pharmacy , a pharmaceutical medicine degree'\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    #row.append('Expertise')\n",
    "                    all.append(row)\n",
    "                 \n",
    "                    for row in reader:\n",
    "                            temp = []\n",
    "                            if row[13].strip() != '-':\n",
    "                                \n",
    "                                if row[13].strip() == 'Refer column R':\n",
    "                                    affs = row[17].split('\\n')\n",
    "                                else:\n",
    "                                    affs = row[13].split('\\n')\n",
    "                                    \n",
    "                                for name in affs:\n",
    "                                    if name.strip() != '':\n",
    "                                        done = 0\n",
    "                                        for i in name.split():\n",
    "                                            #print (i)\n",
    "                                            if i.strip() in expertise.keys():\n",
    "                                                temp.append(i+' : '+expertise[i])\n",
    "                                                done = 1\n",
    "                                                break\n",
    "                                               # print(i)\n",
    "\n",
    "                                        if done == 0:\n",
    "                                                temp.append('-')\n",
    "                            \n",
    "                            else:\n",
    "                                temp.append('-')\n",
    "                                \n",
    "                            row[29] = (\"\\n\".join(temp))\n",
    "                            all.append(row)\n",
    "                            \n",
    "                            #break\n",
    "\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "# GPO Plumbook\n",
    "# Column: \"Type of Appt., Title\"\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "House = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/house_assignments_103-115-3.csv\"\n",
    "Senate = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/senate_assignments_103-115-3.csv\"\n",
    "\n",
    "\n",
    "GPO = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/GPO PlumBooks/GPO-PLUMBOOK-2016/xls/\"\n",
    "\n",
    "GPODict = defaultdict(list)\n",
    "\n",
    "files = set(os.listdir(GPO))\n",
    "\n",
    "for file in files:\n",
    "    #file=file.replace('.csv','')\n",
    "    with open(GPO+file,'r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                GPODict[row[0].lower().strip()].append(row[7].lower().strip()+' :: '+ row[6]+' :: '+row[8])\n",
    "                                \n",
    "print(list(GPODict.keys())[0:15])\n",
    "     \n",
    "print(list(GPODict.values())[0:15])\n",
    "\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    row.append('From GPO Plumbook')\n",
    "                    all.append(row)\n",
    "                 \n",
    "                    for row in reader:\n",
    "                            temp = []\n",
    "                            \n",
    "                            for agency in row[18].split('\\n'):\n",
    "                                if agency.lower().strip() in GPODict.keys():\n",
    "                                    if row[13].strip() != \"Refer column R\":\n",
    "                                        index = row[18].split('\\n').index(agency)\n",
    "                                        #for witness in row[13].split('\\n'):\n",
    "                                        witness = row[13].split('\\n')[index]\n",
    "                                        for item in GPODict[agency.lower().strip()]:\n",
    "                                                done = 0\n",
    "                                                name = item.split(' :: ')[0].lower().strip()\n",
    "                                                if fuzz.token_sort_ratio(( witness.lower().split(':')[0]), name) > 80 and witness.strip()!='':\n",
    "                                                    appt = item.split(' :: ')[2].strip()\n",
    "                                                    title = item.split(' :: ')[1].strip()\n",
    "                                                    temp.append(title +' :: '+appt)\n",
    "                                                    done = 1\n",
    "                                                    break\n",
    "                                        if done == 0:\n",
    "                                                    temp.append('-')\n",
    "                                    else:\n",
    "                                        index = row[18].split('\\n').index(agency)\n",
    "                                        #for witness in row[13].split('\\n'):\n",
    "                                        witness = row[17].split('\\n')[index]\n",
    "                                        for item in GPODict[agency.lower().strip()]:\n",
    "                                                done = 0\n",
    "                                                name = item.split(' :: ')[0].lower().strip()\n",
    "                                                if fuzz.token_sort_ratio(( witness.lower().split()[:4]), name) > 80 and witness.strip()!='':\n",
    "                                                    appt = item.split(' :: ')[2].strip()\n",
    "                                                    title = item.split(' :: ')[1].strip()\n",
    "                                                    temp.append(title +' :: '+appt)\n",
    "                                                    done = 1\n",
    "                                                    break\n",
    "                                        if done == 0:\n",
    "                                                    temp.append('-')\n",
    "                                        \n",
    "                                else:\n",
    "                                    temp.append('-')\n",
    "                                    \n",
    "                            row.append(\"\\n\".join(temp))\n",
    "                            all.append(row)\n",
    "                            \n",
    "                            #break\n",
    "\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender, to find unique names\n",
    "# Individual CSVs\n",
    "\n",
    "namesDict = {}\n",
    "\n",
    "for file in set(os.listdir(results_csvs)):\n",
    "            \n",
    "            file = file.replace('.csv','')\n",
    "            \n",
    "            with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                if row[17].strip() != 'NA' or row[17].strip() != '-':\n",
    "                                    namesDict[row[17]] = 'M/F'\n",
    "                                    #print(indCSVkey)\n",
    "         \n",
    "            \n",
    "\n",
    "print(len(namesDict.keys()))\n",
    "print(list(namesDict.values())[0])    \n",
    "\n",
    "count = 0\n",
    "\n",
    "fout = \"namesDict(Mem+Wit).txt\"\n",
    "fo = open(fout, \"w\")\n",
    "\n",
    "for k, v in namesDict.items():\n",
    "    if (str(k).find(',')!=-1 ):\n",
    "        count += 1\n",
    "    else:\n",
    "        fo.write(str(k) +'\\n')\n",
    "        \n",
    "fo.close()\n",
    "\n",
    "print (count)\n",
    "print (len(namesDict.keys()) - count )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta metadata\n",
    "# Witness level info.:\n",
    "\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "House = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/house_assignments_103-115-3.csv\"\n",
    "Senate = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/senate_assignments_103-115-3.csv\"\n",
    "\n",
    "\n",
    "GPO = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/GPO PlumBooks/GPO-PLUMBOOK-2016/xls/\"\n",
    "\n",
    "\n",
    "\n",
    "MetaMetadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/MetaMetadata_results.csv\"\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                with open(MetaMetadata_results, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "                    temp = []\n",
    "                    temp.append('Filename')\n",
    "                    temp.append('Witnesses')\n",
    "                    temp.append('Scrapped witnesses')\n",
    "                    temp.append('Agency')\n",
    "                    temp.append('JK code')\n",
    "                    temp.append('UA code')\n",
    "                    temp.append('Parent UA code')\n",
    "                    temp.append('US State')\n",
    "                    temp.append('Inspector General')\n",
    "                    temp.append('Expertise')\n",
    "                    temp.append('From GPO Plumbook(Title :: Appt)')\n",
    "                    \n",
    "                    all.append(temp)\n",
    "                 \n",
    "                    for row in reader:\n",
    "                            \n",
    "                            if row[13].strip() != '-':\n",
    "                                \n",
    "                                if row[13].strip() == 'Refer column R':\n",
    "                                    affs = row[17].split('\\n')\n",
    "                                else:\n",
    "                                    affs = row[13].split('\\n')\n",
    "                                #all = []  \n",
    "                                for aff in affs:\n",
    "                                    temp = []\n",
    "                            \n",
    "                                    if aff.strip() != '':\n",
    "                                        try:\n",
    "                                            temp.append(row[6])\n",
    "                                            index = affs.index(aff)\n",
    "\n",
    "                                            if row[13].strip() == 'Refer column R':\n",
    "                                                temp.append('-')\n",
    "                                                temp.append(row[17].split('\\n')[index])\n",
    "\n",
    "                                            else:\n",
    "                                                temp.append(row[13].split('\\n')[index])\n",
    "                                                temp.append('-')\n",
    "\n",
    "                                            \n",
    "                                            temp.append(row[18].split('\\n')[index])\n",
    "                                            temp.append(row[19].split('\\n')[index])\n",
    "                                            temp.append(row[20].split('\\n')[index])\n",
    "                                            temp.append(row[21].split('\\n')[index])\n",
    "                                            temp.append(row[22].split('\\n')[index])\n",
    "                                            temp.append(row[23].split('\\n')[index])\n",
    "                                            temp.append(row[29].split('\\n')[index])\n",
    "                                            temp.append(row[30].split('\\n')[index])\n",
    "\n",
    "\n",
    "                                            all.append(temp)\n",
    "                                            \n",
    "                                        except:\n",
    "                                            print (row)\n",
    "                                #writer.writerows(all)            \n",
    "                            #all.append(row)\n",
    "                            #break\n",
    "\n",
    "                    #except:\n",
    "                    #    writer.writerows(all)\n",
    "                    #    continue\n",
    "                    writer.writerows(all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "# Column: Keywords\n",
    "# keywords \"oversight,\" \"investigation,\" or \"budget request\"\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "House = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/house_assignments_103-115-3.csv\"\n",
    "Senate = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/senate_assignments_103-115-3.csv\"\n",
    "\n",
    "\n",
    "GPO = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/GPO PlumBooks/GPO-PLUMBOOK-2016/xls/\"\n",
    "\n",
    "keyWordsDict = {'oversight' : 1,\n",
    "               'investigation' : 1,\n",
    "               'budget request' : 1}\n",
    "\n",
    "with open(metadata_results, 'r', encoding=\"utf8\") as csvinput:\n",
    "    with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "            reader = csv.reader(csvinput)\n",
    "            writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "         \n",
    "        \n",
    "            row = next(reader)\n",
    "    \n",
    "            all = []\n",
    "            row.append('Keywords present')\n",
    "            all.append(row)\n",
    "                \n",
    "            for row in reader:\n",
    "            \n",
    "                            file = row[6].strip()+'.csv'\n",
    "                            cleaned = ''\n",
    "\n",
    "                            if row[15] == 'Yes':\n",
    "                                with open(results_csvs+file,'r', encoding=\"utf8\") as csvinput1:\n",
    "                                                reader1 = csv.reader(csvinput1)\n",
    "                                                row1 = next(reader1)\n",
    "                                                for row1 in reader1:\n",
    "                                                    cleaned += row1[12]\n",
    "                             \n",
    "                           \n",
    "                            done = 0\n",
    "                            for i in cleaned.split():\n",
    "                                    if str(i).lower().strip() in keyWordsDict.keys():\n",
    "                                        row.append('Yes')\n",
    "                                        done = 1\n",
    "                                        break\n",
    "                            if done == 0:\n",
    "                                row.append('No')\n",
    "                                \n",
    "                            all.append(row)\n",
    "                            \n",
    "                  \n",
    "            writer.writerows(all)                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5874\n",
      "5874\n",
      "['115 :: 102 :: Federal Reserve', '115 :: 102 :: Department of Agriculture', '115 :: 102 :: Farm Credit Administration', '115 :: 102 :: Department of the Treasury', '115 :: 104 :: Office of Community Planning and Development', '115 :: 104 :: Federal Highway Administration', '115 :: 106 :: Central Intelligence Agency', '115 :: 106 :: Department of Defense', '115 :: 173 :: Department of Transportation', '115 :: 106 :: Joint Chiefs of Staff']\n"
     ]
    }
   ],
   "source": [
    "# Commitee: Agency : Congress   -> Triplet\n",
    "# CSV1\n",
    "\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "\n",
    "\n",
    "\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV2.csv\"\n",
    "\n",
    "tripletDict = {}\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                   # with open(CSV1,http://localhost:8888/notebooks/Documents/RA%20NLP/RA_NLP.ipynb# 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                      #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                            reader = csv.reader(csvinput)\n",
    "                            row = next(reader)\n",
    "                            for row in reader:\n",
    "                                    triplet = row[2].strip().replace('th','')+' :: '+row[3].strip()\n",
    "                                    for agency in row[18].split('\\n'):\n",
    "                                        if agency.strip()!='' and agency.strip()!='-':\n",
    "                                            triplet += ' :: '+agency\n",
    "                                            if triplet.strip() in tripletDict.keys():\n",
    "                                                tripletDict[triplet.strip()] += 1\n",
    "                                            else:\n",
    "                                                tripletDict[triplet.strip()] = 1\n",
    "                                        \n",
    "                                        triplet = row[2].strip().replace('th','')+' :: '+row[3].strip()\n",
    "                                        \n",
    "print(len(tripletDict.keys()))\n",
    "          \n",
    "print(len(tripletDict.values()))\n",
    "\n",
    "print(list(tripletDict.keys())[0:10])\n",
    "\n",
    "with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "            writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "        \n",
    "            row = []\n",
    "            all = []\n",
    "            row.append('Congress')                       \n",
    "            row.append('Commitee')\n",
    "            row.append('Agency')\n",
    "            row.append('Count')\n",
    "            all.append(row)\n",
    "                \n",
    "            for key in tripletDict.keys():\n",
    "                            row = []\n",
    "                            item = key.split(' :: ')\n",
    "                            row.append(item[0])\n",
    "                            row.append(item[1])\n",
    "                            row.append(item[2])\n",
    "                            row.append(tripletDict[key])\n",
    "                            all.append(row)\n",
    "                            \n",
    "                            \n",
    "                  \n",
    "            writer.writerows(all)                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "# Columns: \t\t\t\n",
    "#           Attendance proportion %\n",
    "#\t\t\tBills\n",
    "#\t\t\tsubpoena\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "results_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs_new/\"\n",
    "\n",
    "sample_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs/\"\n",
    "sample_csvs_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/sample_csvs_new/\"\n",
    "\n",
    "APIs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/APIs/\"\n",
    "\n",
    "House = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/house_assignments_103-115-3.csv\"\n",
    "Senate = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/senate_assignments_103-115-3.csv\"\n",
    "\n",
    "\n",
    "GPO = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/Extras/GPO PlumBooks/GPO-PLUMBOOK-2016/xls/\"\n",
    "\n",
    "\n",
    "                       \n",
    "with open(metadata_results, 'r', encoding=\"utf8\") as csvinput:\n",
    "    with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "            reader = csv.reader(csvinput)\n",
    "            writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "         \n",
    "        \n",
    "            row = next(reader)\n",
    "    \n",
    "            all = []\n",
    "        \n",
    "            row.append('Attendance proportion %')\n",
    "            row.append('Bills')\n",
    "            row.append('subpoena')\n",
    "            \n",
    "            all.append(row)\n",
    "                \n",
    "            for row in reader:\n",
    "            \n",
    "                            file = row[6].strip()+'.csv'\n",
    "                            \n",
    "                    \n",
    "                            try:\n",
    "                                row.append((int(row[25])/int(row[26])) *100)\n",
    "                            except:\n",
    "                                row.append('-')\n",
    "                                \n",
    "                            cleaned = ''\n",
    "                            bills = 0\n",
    "                            if row[15] == 'Yes':\n",
    "                                with open(results_csvs+file,'r', encoding=\"utf8\") as csvinput1:\n",
    "                                                reader1 = csv.reader(csvinput1)\n",
    "                                                row1 = next(reader1)\n",
    "                                                bills = 0\n",
    "                                                for row1 in reader1:\n",
    "                                                    cleaned += row1[12]\n",
    "                                                    if str(row1[-1]).strip() == '1':\n",
    "                                                        bills = 1\n",
    "                             \n",
    "                            row.append(bills)\n",
    "                            \n",
    "                            done = 0\n",
    "                            subpoena = 0\n",
    "                            \n",
    "                            for i in cleaned.split():\n",
    "                                    if 'subpoena' == str(i).lower().strip() :\n",
    "                                        subpoena += 1\n",
    "                                        done = 1\n",
    "                                                                                \n",
    "                            if done == 0:\n",
    "                                row.append('No')\n",
    "                            \n",
    "                            else:\n",
    "                                row.append(subpoena)\n",
    "                                \n",
    "                            all.append(row)\n",
    "                            \n",
    "                  \n",
    "            writer.writerows(all)                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593352\n",
      "01-1995 102 Broadcasting Board of Governors\n",
      "593352\n",
      "['01-1995 102 Broadcasting Board of Governors', '01-1995 102 Commission on Civil Rights', '01-1995 102 Commodities Futures Trading Commission', '01-1995 102 Consumer Product Safety Commission', '01-1995 102 Court Services and Offender Supervision Agency', '01-1995 102 Department of Agriculture', '01-1995 102 Department of Commerce', '01-1995 102 Department of Defense', '01-1995 102 Department of Education', '01-1995 102 Department of Energy']\n"
     ]
    }
   ],
   "source": [
    "# CSV 1: At the hearing level\n",
    "# Comm. - Agency - Month\n",
    "\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV11.csv\"\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "\n",
    "\n",
    "CSV1Dict = {}\n",
    "Attendance = {}\n",
    "\n",
    "utteranceCount = []\n",
    "\n",
    "with open(CSV1, 'r', encoding=\"utf8\") as csvinput2:\n",
    "    CSV1reader = csv.reader(csvinput2)\n",
    "    \n",
    "    #all = []\n",
    "    CSV1row = next(CSV1reader)\n",
    "\n",
    "    #CSV1row.append('Number of utterances made by the agency about a bill per month')\n",
    "    #all.append(CSV1row)\n",
    "                \n",
    "    for CSV1row in CSV1reader:\n",
    "        count = 0\n",
    "        \n",
    "        CSV1RowDate = str(CSV1row[0])\n",
    "        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "                \n",
    "        CSV1key =  CSV1RowDate+' '+ CSV1row[1] +' '+ CSV1row[2].strip() \n",
    "        \n",
    "        CSV1Dict[str(CSV1key.strip())] = 0\n",
    "        \n",
    "        Attendance[str(CSV1key.strip())]  = 0\n",
    "\n",
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.keys())[0])\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                                reader = csv.reader(csvinput)\n",
    "                                row = next(reader)\n",
    "                                for row in reader:\n",
    "                                        if row[16]!='-' and row[-3]!='-' and str(row[-12].strip())=='Yes' and str(row[-2]).strip()=='0':#and str(row[33].strip()) == '0' and str(row[31].strip())=='Yes':\n",
    "                                            if ';' in row[16].strip():\n",
    "                                                date = row[16].split(';')[0]\n",
    "                                                date = date.split('-')[1]+'-'+date.split('-')[0]\n",
    "                                            else:\n",
    "                                                date = row[16].split('-')[1]+'-'+row[16].split('-')[2]\n",
    "                                       \n",
    "                                            for agency in row[18].split('\\n'):\n",
    "\n",
    "                                                if agency != '':\n",
    "                                                    indCSVkey = date +' '+ row[3].strip() + ' '+ agency.strip()\n",
    "\n",
    "                                                    if indCSVkey.strip() in CSV1Dict.keys():\n",
    "                                                        CSV1Dict[str(indCSVkey.strip())] += 1\n",
    "                                                        Attendance[str(indCSVkey.strip())] += float(row[-3].strip())\n",
    "\n",
    "  \n",
    "            \n",
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.keys())[0:10])               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 1: At the hearing level\n",
    "# Comm. - Agency - Month\n",
    "\n",
    "CSV221 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV221.csv\"\n",
    "\n",
    "monthDict ={'01':'Jan',\n",
    "           '02':'Feb',\n",
    "            '03':'Mar',\n",
    "            '04':'Apr',\n",
    "            '05':'May',\n",
    "            '06':'Jun',\n",
    "            '07':'Jul',\n",
    "            '08':'Aug',\n",
    "            '09':'Sep',\n",
    "            '10':'Oct',\n",
    "            '11':'Nov',\n",
    "            '12':'Dec'\n",
    "            \n",
    "           }\n",
    "with open(CSV1,'r', encoding=\"utf8\") as csvinput:\n",
    "            with open(CSV221, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    #row.append('Count of all hearings')\n",
    "                    #row.append('Count that requires keywords')\n",
    "                    #row.append('Count of only non-bill hearings')\n",
    "                    #row.append('Count of only non-bill hearings that require keywords')\n",
    "                    #row.append('Average # of committee members attending for all hearings')\n",
    "                    #row.append('Average # of committee members attending for all hearings where subcommittee is N/A')\n",
    "                    #row.append('Average # of committee members attending for non-bill hearings')\n",
    "                    #row.append('Average # of committee members attending for non-bill hearings where subcommittee is N/A')\n",
    "                    #row.append('Average # of committee members attending for all hearings where IG is present')\n",
    "                    row.append('Average # of committee members attending for non-bill hearings where IG is present')\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    all.append(row)\n",
    "                    \n",
    "                    for row in reader:\n",
    "                        CSV1RowDate = str(row[0])\n",
    "                        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "                        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "\n",
    "                        CSV1key =  CSV1RowDate+' '+ row[1] +' '+ row[2].strip()\n",
    "                        \n",
    "                        try:\n",
    "                            row.append(float(Attendance[str(CSV1key)] / float(CSV1Dict[str(CSV1key)])))\n",
    "                        except:\n",
    "                            row.append('-')\n",
    "                        \n",
    "                        #row.append(CSV1Dict[str(CSV1key)])\n",
    "                        \n",
    "                        all.append(row)\n",
    "                        \n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 1: At the hearing level\n",
    "# Comm. - Agency - Month\n",
    "\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV11.csv\"\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "\n",
    "\n",
    "CSV1Dict = {}\n",
    "Attendance = {}\n",
    "\n",
    "utteranceCount = []\n",
    "\n",
    "with open(CSV1, 'r', encoding=\"utf8\") as csvinput2:\n",
    "    CSV1reader = csv.reader(csvinput2)\n",
    "    \n",
    "    #all = []\n",
    "    CSV1row = next(CSV1reader)\n",
    "\n",
    "    #CSV1row.append('Number of utterances made by the agency about a bill per month')\n",
    "    #all.append(CSV1row)\n",
    "                \n",
    "    for CSV1row in CSV1reader:\n",
    "        count = 0\n",
    "        \n",
    "        CSV1RowDate = str(CSV1row[0])\n",
    "        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "                \n",
    "        CSV1key =  CSV1RowDate+' '+ CSV1row[1] +' '+ CSV1row[2].strip() \n",
    "        \n",
    "        CSV1Dict[str(CSV1key.strip())] = 0\n",
    "        \n",
    "        Attendance[str(CSV1key.strip())]  = 0\n",
    "\n",
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.keys())[0])\n",
    "\n",
    "with open(metadata_results,'r', encoding=\"utf8\") as csvinput:\n",
    "                                reader = csv.reader(csvinput)\n",
    "                                row = next(reader)\n",
    "                                for row in reader:\n",
    "                                        if row[16]!='-' and row[-3]!='-' and str(row[-12].strip())=='Yes' and str(row[-2]).strip()=='0':#and str(row[33].strip()) == '0' and str(row[31].strip())=='Yes':\n",
    "                                            if ';' in row[16].strip():\n",
    "                                                date = row[16].split(';')[0]\n",
    "                                                date = date.split('-')[1]+'-'+date.split('-')[0]\n",
    "                                            else:\n",
    "                                                date = row[16].split('-')[1]+'-'+row[16].split('-')[2]\n",
    "                                       \n",
    "                                            for agency in row[18].split('\\n'):\n",
    "\n",
    "                                                if agency != '':\n",
    "                                                    indCSVkey = date +' '+ row[3].strip() + ' '+ agency.strip()\n",
    "\n",
    "                                                    if indCSVkey.strip() in CSV1Dict.keys():\n",
    "                                                        CSV1Dict[str(indCSVkey.strip())] += 1\n",
    "                                                        Attendance[str(indCSVkey.strip())] += float(row[-3].strip())\n",
    "\n",
    "  \n",
    "            \n",
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.keys())[0:10])               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593352\n",
      "01-1995 102 Broadcasting Board of Governors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAHUL\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593352\n",
      "['01-1995 102 Broadcasting Board of Governors', '01-1995 102 Commission on Civil Rights', '01-1995 102 Commodities Futures Trading Commission', '01-1995 102 Consumer Product Safety Commission', '01-1995 102 Court Services and Offender Supervision Agency', '01-1995 102 Department of Agriculture', '01-1995 102 Department of Commerce', '01-1995 102 Department of Defense', '01-1995 102 Department of Education', '01-1995 102 Department of Energy']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# CSV 1: At the utterance level\n",
    "# Comm. - Agency - Month\n",
    "\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV11.csv\"\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "\n",
    "\n",
    "CSV1Dict = {}\n",
    "Attendance = {}\n",
    "\n",
    "utteranceCount = []\n",
    "\n",
    "with open(CSV1, 'r', encoding=\"utf8\") as csvinput2:\n",
    "    CSV1reader = csv.reader(csvinput2)\n",
    "    \n",
    "    #all = []\n",
    "    CSV1row = next(CSV1reader)\n",
    "\n",
    "    #CSV1row.append('Number of utterances made by the agency about a bill per month')\n",
    "    #all.append(CSV1row)\n",
    "                \n",
    "    for CSV1row in CSV1reader:\n",
    "        count = 0\n",
    "        \n",
    "        CSV1RowDate = str(CSV1row[0])\n",
    "        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "                \n",
    "        CSV1key =  CSV1RowDate+' '+ CSV1row[1] +' '+ CSV1row[2].strip() \n",
    "        \n",
    "        CSV1Dict[str(CSV1key.strip())] = 0\n",
    "        \n",
    "        Attendance[str(CSV1key.strip())]  = 0\n",
    "\n",
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.keys())[0])\n",
    "\n",
    "df1 = pd.read_csv(metadata_results)\n",
    "for file in set(os.listdir(results_csvs)):\n",
    "            \n",
    "            file = file.replace('.csv','')\n",
    "\n",
    "            index = df1['Filename'].tolist().index(file)\n",
    "            bills = df1['Bills'].tolist()[index]\n",
    "            keywords = df1['Keywords present'].tolist()[index]\n",
    "            attendance = df1['Attendance proportion %'].tolist()[index]\n",
    "            subCommittee = str(df1['subCommittee'].tolist()[index])\n",
    "            IG = str(df1['Inspector General'].tolist()[index])\n",
    "            \n",
    "            if IG=='Yes' and str(bills).strip() == '0':#and subCommittee == '-':# and str(keywords).strip() == 'Yes':\n",
    "                                                        \n",
    "                with open(results_csvs+file+'.csv','r', encoding=\"utf8\") as csvinput:\n",
    "                       # with open(CSV1, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                          #      writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                                reader = csv.reader(csvinput)\n",
    "                                row = next(reader)\n",
    "                                for row in reader:\n",
    "\n",
    "                                        if row[13]!='-' and row[-7]!='-':# and str(row[-12].strip())=='Yes' and str(row[-2]).strip()=='0':#and str(row[33].strip()) == '0' and str(row[31].strip())=='Yes':\n",
    "\n",
    "                                                    date = row[13].split('-')\n",
    "                                                    date = date[0]+'-'+date[2]\n",
    "                                                    indCSVkey = date +' '+ row[0].strip() + ' '+ row[-7].strip()\n",
    "\n",
    "                                                    if indCSVkey.strip() in CSV1Dict.keys():\n",
    "                                                            CSV1Dict[str(indCSVkey.strip())] += 1\n",
    "                                                            Attendance[str(indCSVkey.strip())] += float(attendance)\n",
    "\n",
    "  \n",
    "            \n",
    "print(len(CSV1Dict.keys()))\n",
    "print(list(CSV1Dict.keys())[0:10])   \n",
    "\n",
    "print(list(CSV1Dict.values())[0:10])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 1: At the hearing level\n",
    "# Comm. - Agency - Month\n",
    "\n",
    "CSV221 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV221.csv\"\n",
    "\n",
    "monthDict ={'01':'Jan',\n",
    "           '02':'Feb',\n",
    "            '03':'Mar',\n",
    "            '04':'Apr',\n",
    "            '05':'May',\n",
    "            '06':'Jun',\n",
    "            '07':'Jul',\n",
    "            '08':'Aug',\n",
    "            '09':'Sep',\n",
    "            '10':'Oct',\n",
    "            '11':'Nov',\n",
    "            '12':'Dec'\n",
    "            \n",
    "           }\n",
    "with open(CSV1,'r', encoding=\"utf8\") as csvinput:\n",
    "            with open(CSV221, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "                    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "                    reader = csv.reader(csvinput)\n",
    "\n",
    "                    all = []\n",
    "                    row = next(reader)\n",
    "\n",
    "                    #row.append('Count of all utterances')\n",
    "                    #row.append('Count of all utterances that require keywords')\n",
    "                    #row.append('Count of all utterances for only non-bill hearings')\n",
    "                    #row.append('Count of all utterances for only non-bill hearings that require keywords')\n",
    "                    #row.append('Average # of committee members attending for all hearings at utterance level')\n",
    "                    #row.append('Average # of committee members attending for all hearings where subcommittee is N/A at utterance level')\n",
    "                    #row.append('Average # of committee members attending for non-bill hearings at utterance level')\n",
    "                    #row.append('Average # of committee members attending for non-bill hearings where subcommittee is N/A at utterance level')\n",
    "                    #row.append('Average # of committee members attending for all hearings where IG is present at utterance level')\n",
    "                    row.append('Average # of committee members attending for non-bill hearings where IG is present at utterance level')\n",
    "         \n",
    "                    all.append(row)\n",
    "                    \n",
    "                    for row in reader:\n",
    "                        CSV1RowDate = str(row[0])\n",
    "                        CSV1RowDate = CSV1RowDate.replace('=', '')\n",
    "                        CSV1RowDate = CSV1RowDate.replace('\"', '')\n",
    "\n",
    "                        CSV1key =  CSV1RowDate+' '+ row[1] +' '+ row[2].strip()\n",
    "                        \n",
    "                        try:\n",
    "                            row.append(float(Attendance[str(CSV1key)] / float(CSV1Dict[str(CSV1key)])))\n",
    "                        except:\n",
    "                            row.append('-')\n",
    "                        \n",
    "                        #row.append(CSV1Dict[str(CSV1key)])\n",
    "                        \n",
    "                        all.append(row)\n",
    "                        \n",
    "                    writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MetaMetadata_results : cleaning column B\n",
    "\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "        \n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CSV1 = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/CSV11.csv\"\n",
    "results_csvs = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/results_csvs/\"\n",
    "MetaMetadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/MetaMetadata_results.csv\"\n",
    "MetaMetadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/MetaMetadata_results_new.csv\"\n",
    "\n",
    "\n",
    "CSV1Dict = {}\n",
    "Attendance = {}\n",
    "\n",
    "utteranceCount = []\n",
    "\n",
    "expertise={\n",
    "    \n",
    "    'A.A.' : 'Associate of Arts',\n",
    "\t\t'A.S.' : 'Associate of Science', \n",
    "\t\t'A.A.S.' : 'Associate of Applied Science',\n",
    "\t\t'ADN' : 'Associates Degree in Nursing',\n",
    "\t\t'B.A.' : 'Bachelor of Arts', \n",
    "\t\t'B.S.' : 'Bachelor of Science', \n",
    "\t\t'B.E.' : 'Bachelor of Engineering',\n",
    "\t\t'M.A.' : 'Master of Arts', \n",
    "\t\t'M.S.' : 'Master of Science', \n",
    "\t\t'MBA' : 'Master of Business Administration', \n",
    "\t\t'M.Ed.' : 'Master of Education',\n",
    "\t\t'Ph.D.' : 'Doctor of Philosophy', \n",
    "\t\t'DNP' : 'Doctor of Nursing Practice', \n",
    "\t\t'Ed.D.' : 'Doctor of Education', \n",
    "\t     'J.D.' : 'Juris Doctorate, a law degree',\n",
    "\t\t'M.D.' : 'Medical Doctor, a physicians degree',\n",
    "\t\t'D.D.S.' : 'Doctor of Dental Surgery, a dentistry degree',\n",
    "'Pharm.D.' : 'Doctor of Pharmacy , a pharmaceutical medicine degree'\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "with open(MetaMetadata_results, 'r', encoding=\"utf8\") as csvinput:\n",
    "    with open(MetaMetadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "            reader = csv.reader(csvinput)\n",
    "            writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "        \n",
    "            \n",
    "            all = []\n",
    "            \n",
    "            row = next(reader)\n",
    "            all.append(row)\n",
    "        \n",
    "            for row in reader:\n",
    "            \n",
    "                            row[1]=row[1].split(':')[0]\n",
    "                \n",
    "                            for word in row[1].split():\n",
    "                                if word in expertise.keys():\n",
    "                                    row[1] = row[1].replace(word, '')\n",
    "                                \n",
    "                            all.append(row)\n",
    "                            \n",
    "                  \n",
    "            writer.writerows(all)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metadata_results_new = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results_new.csv\"\n",
    "metadata_results = \"D:/USC/RA NLP/Hearing data/congressional_hearings/congressional_hearings_from_server/gpo_tools/metadata_results.csv\"\n",
    "\n",
    "\n",
    "with open(metadata_results, 'r', encoding=\"utf8\") as csvinput:\n",
    "    with open(metadata_results_new, 'w+', encoding=\"utf8\") as csvoutput:\n",
    "            reader = csv.reader(csvinput)\n",
    "            writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "        \n",
    "            \n",
    "            all = []\n",
    "            \n",
    "            row = next(reader)\n",
    "            all.append(row)\n",
    "        \n",
    "            for row in reader:\n",
    "            \n",
    "                            if row[-11].strip() != '-':\n",
    "                                row[-3] = '-'\n",
    "                    \n",
    "                            all.append(row)\n",
    "                            \n",
    "                  \n",
    "            writer.writerows(all)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
